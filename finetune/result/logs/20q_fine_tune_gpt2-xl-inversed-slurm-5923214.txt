/work/anshitagupta_umass_edu/miniconda3/envs/memit/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 2020
  Num Epochs = 2
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 64
Loaded dataset with 2020 elements
Loaded dataset with 496 elements
  0%|          | 0/64 [00:00<?, ?it/s]  2%|▏         | 1/64 [00:02<02:10,  2.07s/it]  3%|▎         | 2/64 [00:03<01:38,  1.60s/it]  5%|▍         | 3/64 [00:04<01:28,  1.44s/it]  6%|▋         | 4/64 [00:05<01:22,  1.37s/it]  8%|▊         | 5/64 [00:07<01:18,  1.34s/it]  9%|▉         | 6/64 [00:08<01:16,  1.32s/it] 11%|█         | 7/64 [00:09<01:14,  1.31s/it] 12%|█▎        | 8/64 [00:10<01:12,  1.30s/it] 14%|█▍        | 9/64 [00:12<01:11,  1.30s/it] 16%|█▌        | 10/64 [00:13<01:09,  1.29s/it] 17%|█▋        | 11/64 [00:14<01:08,  1.29s/it] 19%|█▉        | 12/64 [00:16<01:07,  1.29s/it] 20%|██        | 13/64 [00:17<01:06,  1.30s/it] 22%|██▏       | 14/64 [00:18<01:04,  1.30s/it] 23%|██▎       | 15/64 [00:20<01:03,  1.30s/it] 25%|██▌       | 16/64 [00:21<01:02,  1.30s/it] 27%|██▋       | 17/64 [00:22<01:01,  1.30s/it] 28%|██▊       | 18/64 [00:23<00:59,  1.30s/it] 30%|██▉       | 19/64 [00:25<00:58,  1.30s/it] 31%|███▏      | 20/64 [00:26<00:57,  1.31s/it] 33%|███▎      | 21/64 [00:27<00:56,  1.31s/it] 34%|███▍      | 22/64 [00:29<00:55,  1.31s/it] 36%|███▌      | 23/64 [00:30<00:53,  1.31s/it] 38%|███▊      | 24/64 [00:31<00:52,  1.32s/it] 39%|███▉      | 25/64 [00:33<00:51,  1.32s/it] 41%|████      | 26/64 [00:34<00:50,  1.32s/it] 42%|████▏     | 27/64 [00:35<00:49,  1.33s/it] 44%|████▍     | 28/64 [00:37<00:47,  1.33s/it] 45%|████▌     | 29/64 [00:38<00:46,  1.33s/it] 47%|████▋     | 30/64 [00:39<00:45,  1.33s/it] 48%|████▊     | 31/64 [00:41<00:44,  1.33s/it] 50%|█████     | 32/64 [00:42<00:38,  1.22s/it]***** Running Evaluation *****
  Num examples = 496
  Batch size = 64

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:01,  5.78it/s][A
 38%|███▊      | 3/8 [00:00<00:01,  4.07it/s][A
 50%|█████     | 4/8 [00:01<00:01,  3.52it/s][A
 62%|██████▎   | 5/8 [00:01<00:00,  3.26it/s][A
 75%|███████▌  | 6/8 [00:01<00:00,  3.11it/s][A
 88%|████████▊ | 7/8 [00:02<00:00,  3.03it/s][A
100%|██████████| 8/8 [00:02<00:00,  3.19it/s][A                                               
                                             [A 50%|█████     | 32/64 [00:44<00:38,  1.22s/it]
100%|██████████| 8/8 [00:02<00:00,  3.19it/s][A
                                             [ASaving model checkpoint to results/checkpoints/20q/gpt2-xl/inversed/checkpoint-32
Configuration saved in results/checkpoints/20q/gpt2-xl/inversed/checkpoint-32/config.json
Model weights saved in results/checkpoints/20q/gpt2-xl/inversed/checkpoint-32/pytorch_model.bin
 52%|█████▏    | 33/64 [03:49<29:29, 57.07s/it] 53%|█████▎    | 34/64 [03:50<20:09, 40.33s/it] 55%|█████▍    | 35/64 [03:52<13:49, 28.61s/it] 56%|█████▋    | 36/64 [03:53<09:31, 20.41s/it] 58%|█████▊    | 37/64 [03:54<06:36, 14.67s/it] 59%|█████▉    | 38/64 [03:55<04:37, 10.66s/it] 61%|██████    | 39/64 [03:57<03:16,  7.85s/it] 62%|██████▎   | 40/64 [03:58<02:21,  5.88s/it] 64%|██████▍   | 41/64 [03:59<01:43,  4.51s/it] 66%|██████▌   | 42/64 [04:01<01:17,  3.54s/it] 67%|██████▋   | 43/64 [04:02<01:00,  2.87s/it] 69%|██████▉   | 44/64 [04:03<00:48,  2.40s/it] 70%|███████   | 45/64 [04:05<00:39,  2.08s/it] 72%|███████▏  | 46/64 [04:06<00:33,  1.85s/it] 73%|███████▎  | 47/64 [04:07<00:28,  1.69s/it] 75%|███████▌  | 48/64 [04:08<00:25,  1.58s/it] 77%|███████▋  | 49/64 [04:10<00:22,  1.50s/it] 78%|███████▊  | 50/64 [04:11<00:20,  1.44s/it] 80%|███████▉  | 51/64 [04:12<00:18,  1.41s/it] 81%|████████▏ | 52/64 [04:14<00:16,  1.38s/it] 83%|████████▎ | 53/64 [04:15<00:14,  1.36s/it] 84%|████████▍ | 54/64 [04:16<00:13,  1.35s/it] 86%|████████▌ | 55/64 [04:18<00:12,  1.35s/it] 88%|████████▊ | 56/64 [04:19<00:10,  1.34s/it] 89%|████████▉ | 57/64 [04:20<00:09,  1.34s/it] 91%|█████████ | 58/64 [04:22<00:08,  1.34s/it] 92%|█████████▏| 59/64 [04:23<00:06,  1.34s/it] 94%|█████████▍| 60/64 [04:24<00:05,  1.34s/it] 95%|█████████▌| 61/64 [04:26<00:04,  1.34s/it] 97%|█████████▋| 62/64 [04:27<00:02,  1.34s/it] 98%|█████████▊| 63/64 [04:28<00:01,  1.35s/it]100%|██████████| 64/64 [04:29<00:00,  1.22s/it]***** Running Evaluation *****
  Num examples = 496
  Batch size = 64
{'eval_loss': 1.0375351905822754, 'eval_runtime': 2.7131, 'eval_samples_per_second': 182.819, 'eval_steps_per_second': 2.949, 'epoch': 1.0}

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|██▌       | 2/8 [00:00<00:01,  5.77it/s][A
 38%|███▊      | 3/8 [00:00<00:01,  4.05it/s][A
 50%|█████     | 4/8 [00:01<00:01,  3.50it/s][A
 62%|██████▎   | 5/8 [00:01<00:00,  3.24it/s][A
 75%|███████▌  | 6/8 [00:01<00:00,  3.10it/s][A
 88%|████████▊ | 7/8 [00:02<00:00,  3.02it/s][A
100%|██████████| 8/8 [00:02<00:00,  3.17it/s][A                                               
                                             [A100%|██████████| 64/64 [04:32<00:00,  1.22s/it]
100%|██████████| 8/8 [00:02<00:00,  3.17it/s][A
                                             [ASaving model checkpoint to results/checkpoints/20q/gpt2-xl/inversed/checkpoint-64
Configuration saved in results/checkpoints/20q/gpt2-xl/inversed/checkpoint-64/config.json
Model weights saved in results/checkpoints/20q/gpt2-xl/inversed/checkpoint-64/pytorch_model.bin


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from results/checkpoints/20q/gpt2-xl/inversed/checkpoint-64 (score: 1.0350630283355713).
                                               100%|██████████| 64/64 [09:23<00:00,  1.22s/it]100%|██████████| 64/64 [09:23<00:00,  8.80s/it]
***** Running Evaluation *****
  Num examples = 496
  Batch size = 64
{'eval_loss': 1.0350630283355713, 'eval_runtime': 2.7168, 'eval_samples_per_second': 182.566, 'eval_steps_per_second': 2.945, 'epoch': 2.0}
{'train_runtime': 563.0964, 'train_samples_per_second': 7.175, 'train_steps_per_second': 0.114, 'train_loss': 1.181452751159668, 'epoch': 2.0}
  0%|          | 0/8 [00:00<?, ?it/s] 25%|██▌       | 2/8 [00:00<00:00,  6.34it/s] 38%|███▊      | 3/8 [00:00<00:01,  4.50it/s] 50%|█████     | 4/8 [00:00<00:01,  3.88it/s] 62%|██████▎   | 5/8 [00:01<00:00,  3.59it/s] 75%|███████▌  | 6/8 [00:01<00:00,  3.44it/s] 88%|████████▊ | 7/8 [00:01<00:00,  3.33it/s]100%|██████████| 8/8 [00:02<00:00,  3.50it/s]100%|██████████| 8/8 [00:02<00:00,  3.69it/s]
Saving model checkpoint to results/best-checkpoints/20q/gpt2-xl/inversed
Configuration saved in results/best-checkpoints/20q/gpt2-xl/inversed/config.json
Model weights saved in results/best-checkpoints/20q/gpt2-xl/inversed/pytorch_model.bin
{'eval_loss': 1.0350630283355713, 'eval_runtime': 2.462, 'eval_samples_per_second': 201.46, 'eval_steps_per_second': 3.249, 'epoch': 2.0}
Loaded dataset with 2020 elements
Loaded dataset with 496 elements
Loaded dataset with 2520 elements
train Accuracy =  0.8267326732673267
train F1 score =  0.8903234014990284
train Confusion Matrix =  [[714  38]
 [  4 956]]
Classification Report               precision    recall  f1-score   support

        6407       0.99      0.72      0.83       994
       10352       0.96      0.93      0.95      1026

   micro avg       0.98      0.83      0.89      2020
   macro avg       0.98      0.83      0.89      2020
weighted avg       0.98      0.83      0.89      2020

valid Accuracy =  0.8649193548387096
valid F1 score =  0.9194805194805193
valid Confusion Matrix =  [[186   4]
 [  0 243]]
Classification Report               precision    recall  f1-score   support

        6407       1.00      0.75      0.86       248
       10352       0.98      0.98      0.98       248

   micro avg       0.99      0.86      0.92       496
   macro avg       0.99      0.86      0.92       496
weighted avg       0.99      0.86      0.92       496

test Accuracy =  0.8376984126984127
test F1 score =  0.8983013682932366
test Confusion Matrix =  [[ 917   46]
 [   3 1194]]
Classification Report               precision    recall  f1-score   support

        6407       1.00      0.74      0.85      1246
       10352       0.96      0.94      0.95      1274

   micro avg       0.98      0.84      0.90      2520
   macro avg       0.98      0.84      0.90      2520
weighted avg       0.98      0.84      0.90      2520

