/work/anshitagupta_umass_edu/miniconda3/envs/memit/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 2040
  Num Epochs = 2
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 64
Loaded dataset with 2040 elements
Loaded dataset with 508 elements
  0%|          | 0/64 [00:00<?, ?it/s]  2%|â–         | 1/64 [00:02<02:10,  2.07s/it]  3%|â–Ž         | 2/64 [00:03<01:39,  1.60s/it]  5%|â–         | 3/64 [00:04<01:28,  1.45s/it]  6%|â–‹         | 4/64 [00:05<01:22,  1.38s/it]  8%|â–Š         | 5/64 [00:07<01:19,  1.34s/it]  9%|â–‰         | 6/64 [00:08<01:16,  1.33s/it] 11%|â–ˆ         | 7/64 [00:09<01:14,  1.31s/it] 12%|â–ˆâ–Ž        | 8/64 [00:11<01:13,  1.31s/it] 14%|â–ˆâ–        | 9/64 [00:12<01:11,  1.30s/it] 16%|â–ˆâ–Œ        | 10/64 [00:13<01:10,  1.30s/it] 17%|â–ˆâ–‹        | 11/64 [00:14<01:09,  1.30s/it] 19%|â–ˆâ–‰        | 12/64 [00:16<01:07,  1.30s/it] 20%|â–ˆâ–ˆ        | 13/64 [00:17<01:06,  1.30s/it] 22%|â–ˆâ–ˆâ–       | 14/64 [00:18<01:05,  1.30s/it] 23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:20<01:03,  1.30s/it] 25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:21<01:02,  1.30s/it] 27%|â–ˆâ–ˆâ–‹       | 17/64 [00:22<01:01,  1.31s/it] 28%|â–ˆâ–ˆâ–Š       | 18/64 [00:24<01:00,  1.31s/it] 30%|â–ˆâ–ˆâ–‰       | 19/64 [00:25<00:59,  1.32s/it] 31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:26<00:58,  1.32s/it] 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:28<00:57,  1.33s/it] 34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:29<00:55,  1.33s/it] 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:30<00:54,  1.34s/it] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:32<00:53,  1.34s/it] 39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:33<00:52,  1.34s/it] 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:34<00:50,  1.34s/it] 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:36<00:49,  1.34s/it] 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/64 [00:37<00:48,  1.35s/it] 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 29/64 [00:38<00:47,  1.35s/it] 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 30/64 [00:40<00:45,  1.35s/it] 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 31/64 [00:41<00:44,  1.35s/it] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 32/64 [00:42<00:42,  1.31s/it]***** Running Evaluation *****
  Num examples = 508
  Batch size = 64

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:01,  5.73it/s][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:00<00:01,  4.04it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:01<00:01,  3.50it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:01<00:00,  3.25it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:01<00:00,  3.11it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:02<00:00,  3.03it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:02<00:00,  3.03it/s][A                                               
                                             [A 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 32/64 [00:45<00:42,  1.31s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:02<00:00,  3.03it/s][A
                                             [ASaving model checkpoint to results/checkpoints/20q/gpt2-xl/normal/checkpoint-32
Configuration saved in results/checkpoints/20q/gpt2-xl/normal/checkpoint-32/config.json
Model weights saved in results/checkpoints/20q/gpt2-xl/normal/checkpoint-32/pytorch_model.bin
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 33/64 [04:15<33:28, 64.78s/it] 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 34/64 [04:16<22:51, 45.72s/it] 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 35/64 [04:18<15:39, 32.39s/it] 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 36/64 [04:19<10:45, 23.05s/it] 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 37/64 [04:20<07:26, 16.52s/it] 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 38/64 [04:22<05:10, 11.95s/it] 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 39/64 [04:23<03:38,  8.75s/it] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 40/64 [04:24<02:36,  6.52s/it] 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 41/64 [04:25<01:53,  4.95s/it] 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 42/64 [04:27<01:24,  3.86s/it] 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 43/64 [04:28<01:04,  3.09s/it] 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 44/64 [04:29<00:51,  2.56s/it] 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 45/64 [04:31<00:41,  2.18s/it] 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 46/64 [04:32<00:34,  1.92s/it] 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 47/64 [04:33<00:29,  1.74s/it] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 48/64 [04:35<00:25,  1.61s/it] 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 49/64 [04:36<00:22,  1.52s/it] 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 50/64 [04:37<00:20,  1.46s/it] 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 51/64 [04:39<00:18,  1.42s/it] 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 52/64 [04:40<00:16,  1.39s/it] 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 53/64 [04:41<00:15,  1.37s/it] 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 54/64 [04:43<00:13,  1.36s/it] 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 55/64 [04:44<00:12,  1.35s/it] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 56/64 [04:45<00:10,  1.35s/it] 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 57/64 [04:47<00:09,  1.35s/it] 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 58/64 [04:48<00:08,  1.35s/it] 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 59/64 [04:49<00:06,  1.35s/it] 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 60/64 [04:51<00:05,  1.35s/it] 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 61/64 [04:52<00:04,  1.35s/it] 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 62/64 [04:53<00:02,  1.35s/it] 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 63/64 [04:55<00:01,  1.36s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [04:56<00:00,  1.32s/it]***** Running Evaluation *****
  Num examples = 508
  Batch size = 64
{'eval_loss': 1.0311331748962402, 'eval_runtime': 2.7672, 'eval_samples_per_second': 183.576, 'eval_steps_per_second': 2.891, 'epoch': 1.0}

  0%|          | 0/8 [00:00<?, ?it/s][A
 25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:01,  5.72it/s][A
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:00<00:01,  4.04it/s][A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:01<00:01,  3.48it/s][A
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:01<00:00,  3.23it/s][A
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:01<00:00,  3.09it/s][A
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:02<00:00,  3.00it/s][A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:02<00:00,  3.02it/s][A                                               
                                             [A100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [04:59<00:00,  1.32s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:02<00:00,  3.02it/s][A
                                             [ASaving model checkpoint to results/checkpoints/20q/gpt2-xl/normal/checkpoint-64
Configuration saved in results/checkpoints/20q/gpt2-xl/normal/checkpoint-64/config.json
Model weights saved in results/checkpoints/20q/gpt2-xl/normal/checkpoint-64/pytorch_model.bin


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from results/checkpoints/20q/gpt2-xl/normal/checkpoint-32 (score: 1.0311331748962402).
                                               100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [10:03<00:00,  1.32s/it]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [10:03<00:00,  9.43s/it]
***** Running Evaluation *****
  Num examples = 508
  Batch size = 64
{'eval_loss': 1.0366102457046509, 'eval_runtime': 2.7783, 'eval_samples_per_second': 182.846, 'eval_steps_per_second': 2.879, 'epoch': 2.0}
{'train_runtime': 603.258, 'train_samples_per_second': 6.763, 'train_steps_per_second': 0.106, 'train_loss': 1.145586371421814, 'epoch': 2.0}
  0%|          | 0/8 [00:00<?, ?it/s] 25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:00,  6.46it/s] 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:00<00:01,  4.54it/s] 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:00<00:01,  3.91it/s] 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:01<00:00,  3.63it/s] 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:01<00:00,  3.47it/s] 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:01<00:00,  3.36it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:02<00:00,  3.36it/s]100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:02<00:00,  3.64it/s]
Saving model checkpoint to results/best-checkpoints/20q/gpt2-xl/normal
Configuration saved in results/best-checkpoints/20q/gpt2-xl/normal/config.json
Model weights saved in results/best-checkpoints/20q/gpt2-xl/normal/pytorch_model.bin
{'eval_loss': 1.0311331748962402, 'eval_runtime': 2.4878, 'eval_samples_per_second': 204.196, 'eval_steps_per_second': 3.216, 'epoch': 2.0}
Loaded dataset with 2040 elements
Loaded dataset with 508 elements
Loaded dataset with 2535 elements
train Accuracy =  0.12696078431372548
train F1 score =  0.2238955868920104
train Confusion Matrix =  [[ 97   0]
 [  0 162]]
Classification Report               precision    recall  f1-score   support

        6407       1.00      0.10      0.17      1020
       10352       1.00      0.16      0.27      1020

   micro avg       1.00      0.13      0.23      2040
   macro avg       1.00      0.13      0.22      2040
weighted avg       1.00      0.13      0.22      2040

valid Accuracy =  0.12401574803149606
valid F1 score =  0.21974272106839499
valid Confusion Matrix =  [[38  0]
 [ 0 25]]
Classification Report               precision    recall  f1-score   support

        6407       1.00      0.15      0.26       254
       10352       1.00      0.10      0.18       254

   micro avg       1.00      0.12      0.22       508
   macro avg       1.00      0.12      0.22       508
weighted avg       1.00      0.12      0.22       508

test Accuracy =  0.11203155818540433
test F1 score =  0.2005108807139264
test Confusion Matrix =  [[115   0]
 [  3 169]]
Classification Report               precision    recall  f1-score   support

        6407       0.97      0.09      0.17      1261
       10352       1.00      0.13      0.23      1274

   micro avg       0.99      0.11      0.20      2535
   macro avg       0.99      0.11      0.20      2535
weighted avg       0.99      0.11      0.20      2535

