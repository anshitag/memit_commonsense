/work/anshitagupta_umass_edu/miniconda3/envs/memit/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 2020
  Num Epochs = 2
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 64
Loaded dataset with 2020 elements
Loaded dataset with 496 elements

  0%|          | 0/64 [00:00<?, ?it/s]
  2%|▏         | 1/64 [00:02<02:07,  2.03s/it]
  3%|▎         | 2/64 [00:03<01:54,  1.84s/it]
  5%|▍         | 3/64 [00:05<01:48,  1.79s/it]
  6%|▋         | 4/64 [00:07<01:45,  1.76s/it]
  8%|▊         | 5/64 [00:08<01:42,  1.74s/it]
  9%|▉         | 6/64 [00:10<01:40,  1.74s/it]
 11%|█         | 7/64 [00:12<01:38,  1.73s/it]
 12%|█▎        | 8/64 [00:14<01:36,  1.73s/it]
 14%|█▍        | 9/64 [00:15<01:34,  1.73s/it]
 16%|█▌        | 10/64 [00:17<01:33,  1.73s/it]
 17%|█▋        | 11/64 [00:19<01:31,  1.73s/it]
 19%|█▉        | 12/64 [00:20<01:29,  1.72s/it]
 20%|██        | 13/64 [00:22<01:27,  1.72s/it]
 22%|██▏       | 14/64 [00:24<01:26,  1.72s/it]
 23%|██▎       | 15/64 [00:26<01:24,  1.72s/it]
 25%|██▌       | 16/64 [00:27<01:22,  1.73s/it]
 27%|██▋       | 17/64 [00:29<01:21,  1.73s/it]
 28%|██▊       | 18/64 [00:31<01:19,  1.73s/it]
 30%|██▉       | 19/64 [00:33<01:17,  1.73s/it]
 31%|███▏      | 20/64 [00:34<01:15,  1.73s/it]
 33%|███▎      | 21/64 [00:36<01:14,  1.73s/it]
 34%|███▍      | 22/64 [00:38<01:12,  1.73s/it]
 36%|███▌      | 23/64 [00:39<01:10,  1.73s/it]
 38%|███▊      | 24/64 [00:41<01:09,  1.73s/it]
 39%|███▉      | 25/64 [00:43<01:07,  1.73s/it]
 41%|████      | 26/64 [00:45<01:05,  1.73s/it]
 42%|████▏     | 27/64 [00:46<01:03,  1.73s/it]
 44%|████▍     | 28/64 [00:48<01:02,  1.73s/it]
 45%|████▌     | 29/64 [00:50<01:00,  1.73s/it]
 47%|████▋     | 30/64 [00:52<00:58,  1.73s/it]
 48%|████▊     | 31/64 [00:53<00:56,  1.72s/it]
 50%|█████     | 32/64 [00:54<00:50,  1.57s/it]***** Running Evaluation *****
  Num examples = 496
  Batch size = 64


  0%|          | 0/8 [00:00<?, ?it/s][A

 25%|██▌       | 2/8 [00:00<00:01,  4.44it/s][A

 38%|███▊      | 3/8 [00:00<00:01,  3.14it/s][A

 50%|█████     | 4/8 [00:01<00:01,  2.72it/s][A

 62%|██████▎   | 5/8 [00:01<00:01,  2.52it/s][A

 75%|███████▌  | 6/8 [00:02<00:00,  2.42it/s][A

 88%|████████▊ | 7/8 [00:02<00:00,  2.35it/s][A

100%|██████████| 8/8 [00:03<00:00,  2.44it/s][A
                                               

                                             
[A
 50%|█████     | 32/64 [00:58<00:50,  1.57s/it]

100%|██████████| 8/8 [00:03<00:00,  2.44it/s][A

                                             [ASaving model checkpoint to results/checkpoints/20q/gpt2-large/inversed/checkpoint-32
Configuration saved in results/checkpoints/20q/gpt2-large/inversed/checkpoint-32/config.json
Model weights saved in results/checkpoints/20q/gpt2-large/inversed/checkpoint-32/pytorch_model.bin

 52%|█████▏    | 33/64 [03:15<22:17, 43.16s/it]
 53%|█████▎    | 34/64 [03:16<15:21, 30.73s/it]
 55%|█████▍    | 35/64 [03:18<10:38, 22.03s/it]
 56%|█████▋    | 36/64 [03:20<07:26, 15.93s/it]
 58%|█████▊    | 37/64 [03:22<05:15, 11.67s/it]
 59%|█████▉    | 38/64 [03:23<03:45,  8.68s/it]
 61%|██████    | 39/64 [03:25<02:44,  6.59s/it]
 62%|██████▎   | 40/64 [03:27<02:03,  5.13s/it]
 64%|██████▍   | 41/64 [03:28<01:34,  4.11s/it]
 66%|██████▌   | 42/64 [03:30<01:14,  3.39s/it]
 67%|██████▋   | 43/64 [03:32<01:00,  2.89s/it]
 69%|██████▉   | 44/64 [03:34<00:50,  2.54s/it]
 70%|███████   | 45/64 [03:35<00:43,  2.29s/it]
 72%|███████▏  | 46/64 [03:37<00:38,  2.12s/it]
 73%|███████▎  | 47/64 [03:39<00:34,  2.00s/it]
 75%|███████▌  | 48/64 [03:40<00:30,  1.92s/it]
 77%|███████▋  | 49/64 [03:42<00:27,  1.86s/it]
 78%|███████▊  | 50/64 [03:44<00:25,  1.82s/it]
 80%|███████▉  | 51/64 [03:46<00:23,  1.79s/it]
 81%|████████▏ | 52/64 [03:47<00:21,  1.78s/it]
 83%|████████▎ | 53/64 [03:49<00:19,  1.76s/it]
 84%|████████▍ | 54/64 [03:51<00:17,  1.75s/it]
 86%|████████▌ | 55/64 [03:53<00:15,  1.75s/it]
 88%|████████▊ | 56/64 [03:54<00:13,  1.74s/it]
 89%|████████▉ | 57/64 [03:56<00:12,  1.74s/it]
 91%|█████████ | 58/64 [03:58<00:10,  1.74s/it]
 92%|█████████▏| 59/64 [03:59<00:08,  1.74s/it]
 94%|█████████▍| 60/64 [04:01<00:06,  1.74s/it]
 95%|█████████▌| 61/64 [04:03<00:05,  1.73s/it]
 97%|█████████▋| 62/64 [04:05<00:03,  1.73s/it]
 98%|█████████▊| 63/64 [04:06<00:01,  1.73s/it]
100%|██████████| 64/64 [04:08<00:00,  1.58s/it]***** Running Evaluation *****
  Num examples = 496
  Batch size = 64
{'eval_loss': 1.0685625076293945, 'eval_runtime': 3.5276, 'eval_samples_per_second': 140.607, 'eval_steps_per_second': 2.268, 'epoch': 1.0}


  0%|          | 0/8 [00:00<?, ?it/s][A

 25%|██▌       | 2/8 [00:00<00:01,  4.41it/s][A

 38%|███▊      | 3/8 [00:00<00:01,  3.11it/s][A

 50%|█████     | 4/8 [00:01<00:01,  2.69it/s][A

 62%|██████▎   | 5/8 [00:01<00:01,  2.50it/s][A

 75%|███████▌  | 6/8 [00:02<00:00,  2.39it/s][A

 88%|████████▊ | 7/8 [00:02<00:00,  2.33it/s][A

100%|██████████| 8/8 [00:03<00:00,  2.41it/s][A
                                               

                                             
[A
100%|██████████| 64/64 [04:11<00:00,  1.58s/it]

100%|██████████| 8/8 [00:03<00:00,  2.41it/s][A

                                             [ASaving model checkpoint to results/checkpoints/20q/gpt2-large/inversed/checkpoint-64
Configuration saved in results/checkpoints/20q/gpt2-large/inversed/checkpoint-64/config.json
Model weights saved in results/checkpoints/20q/gpt2-large/inversed/checkpoint-64/pytorch_model.bin
Deleting older checkpoint [results/checkpoints/20q/gpt2-large/inversed/checkpoint-16] due to args.save_total_limit


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from results/checkpoints/20q/gpt2-large/inversed/checkpoint-64 (score: 1.0386970043182373).

                                               

100%|██████████| 64/64 [06:12<00:00,  1.58s/it]
100%|██████████| 64/64 [06:12<00:00,  5.82s/it]
***** Running Evaluation *****
  Num examples = 496
  Batch size = 64
{'eval_loss': 1.0386970043182373, 'eval_runtime': 3.5642, 'eval_samples_per_second': 139.16, 'eval_steps_per_second': 2.245, 'epoch': 2.0}
{'train_runtime': 372.5908, 'train_samples_per_second': 10.843, 'train_steps_per_second': 0.172, 'train_loss': 1.310753345489502, 'epoch': 2.0}

  0%|          | 0/8 [00:00<?, ?it/s]
 25%|██▌       | 2/8 [00:00<00:01,  4.44it/s]
 38%|███▊      | 3/8 [00:00<00:01,  3.13it/s]
 50%|█████     | 4/8 [00:01<00:01,  2.72it/s]
 62%|██████▎   | 5/8 [00:01<00:01,  2.52it/s]
 75%|███████▌  | 6/8 [00:02<00:00,  2.41it/s]
 88%|████████▊ | 7/8 [00:02<00:00,  2.35it/s]
100%|██████████| 8/8 [00:03<00:00,  2.43it/s]
100%|██████████| 8/8 [00:03<00:00,  2.57it/s]
Saving model checkpoint to results/best-checkpoints/20q/gpt2-large/inversed
Configuration saved in results/best-checkpoints/20q/gpt2-large/inversed/config.json
Model weights saved in results/best-checkpoints/20q/gpt2-large/inversed/pytorch_model.bin
{'eval_loss': 1.0386970043182373, 'eval_runtime': 3.5248, 'eval_samples_per_second': 140.716, 'eval_steps_per_second': 2.27, 'epoch': 2.0}
Loaded dataset with 2020 elements
Loaded dataset with 496 elements
Loaded dataset with 2520 elements
train Accuracy =  0.22326732673267327
train F1 score =  0.24140714678908495
train Confusion Matrix = 
 [[183   3 808]
 [  0 268 758]
 [  0   0   0]]
Classification Report: 
               precision    recall  f1-score   support

        True       1.00      0.18      0.31       994
       False       0.99      0.26      0.41      1026
        None       0.00      1.00      0.00         0

    accuracy                           0.22      2020
   macro avg       0.66      0.48      0.24      2020
weighted avg       0.99      0.22      0.36      2020

valid Accuracy =  0.23387096774193547
valid F1 score =  0.2497608338687799
valid Confusion Matrix = 
 [[ 42   0 206]
 [  0  74 174]
 [  0   0   0]]
Classification Report: 
               precision    recall  f1-score   support

        True       1.00      0.17      0.29       248
       False       1.00      0.30      0.46       248
        None       0.00      1.00      0.00         0

    accuracy                           0.23       496
   macro avg       0.67      0.49      0.25       496
weighted avg       1.00      0.23      0.37       496

test Accuracy =  0.22341269841269842
test F1 score =  0.24008860660236808
test Confusion Matrix = 
 [[ 210    8 1028]
 [   0  353  921]
 [   0    0    0]]
Classification Report: 
               precision    recall  f1-score   support

        True       1.00      0.17      0.29      1246
       False       0.98      0.28      0.43      1274
        None       0.00      1.00      0.00         0

    accuracy                           0.22      2520
   macro avg       0.66      0.48      0.24      2520
weighted avg       0.99      0.22      0.36      2520

