/work/anshitagupta_umass_edu/miniconda3/envs/memit/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 2020
  Num Epochs = 2
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 64
Loaded dataset with 2020 elements
Loaded dataset with 496 elements

  0%|          | 0/64 [00:00<?, ?it/s]
  2%|â–         | 1/64 [00:02<02:07,  2.03s/it]
  3%|â–Ž         | 2/64 [00:03<01:54,  1.84s/it]
  5%|â–         | 3/64 [00:05<01:48,  1.79s/it]
  6%|â–‹         | 4/64 [00:07<01:45,  1.76s/it]
  8%|â–Š         | 5/64 [00:08<01:42,  1.74s/it]
  9%|â–‰         | 6/64 [00:10<01:40,  1.74s/it]
 11%|â–ˆ         | 7/64 [00:12<01:38,  1.73s/it]
 12%|â–ˆâ–Ž        | 8/64 [00:14<01:36,  1.73s/it]
 14%|â–ˆâ–        | 9/64 [00:15<01:34,  1.73s/it]
 16%|â–ˆâ–Œ        | 10/64 [00:17<01:33,  1.73s/it]
 17%|â–ˆâ–‹        | 11/64 [00:19<01:31,  1.73s/it]
 19%|â–ˆâ–‰        | 12/64 [00:20<01:29,  1.72s/it]
 20%|â–ˆâ–ˆ        | 13/64 [00:22<01:27,  1.72s/it]
 22%|â–ˆâ–ˆâ–       | 14/64 [00:24<01:26,  1.72s/it]
 23%|â–ˆâ–ˆâ–Ž       | 15/64 [00:26<01:24,  1.72s/it]
 25%|â–ˆâ–ˆâ–Œ       | 16/64 [00:27<01:22,  1.73s/it]
 27%|â–ˆâ–ˆâ–‹       | 17/64 [00:29<01:21,  1.73s/it]
 28%|â–ˆâ–ˆâ–Š       | 18/64 [00:31<01:19,  1.73s/it]
 30%|â–ˆâ–ˆâ–‰       | 19/64 [00:33<01:17,  1.73s/it]
 31%|â–ˆâ–ˆâ–ˆâ–      | 20/64 [00:34<01:15,  1.73s/it]
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 21/64 [00:36<01:14,  1.73s/it]
 34%|â–ˆâ–ˆâ–ˆâ–      | 22/64 [00:38<01:12,  1.73s/it]
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 23/64 [00:39<01:10,  1.73s/it]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 24/64 [00:41<01:09,  1.73s/it]
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 25/64 [00:43<01:07,  1.73s/it]
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 26/64 [00:45<01:05,  1.73s/it]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 27/64 [00:46<01:03,  1.73s/it]
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 28/64 [00:48<01:02,  1.73s/it]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 29/64 [00:50<01:00,  1.73s/it]
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 30/64 [00:52<00:58,  1.73s/it]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 31/64 [00:53<00:56,  1.72s/it]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 32/64 [00:54<00:50,  1.57s/it]***** Running Evaluation *****
  Num examples = 496
  Batch size = 64


  0%|          | 0/8 [00:00<?, ?it/s][A

 25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:01,  4.44it/s][A

 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:00<00:01,  3.14it/s][A

 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:01<00:01,  2.72it/s][A

 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:01<00:01,  2.52it/s][A

 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:02<00:00,  2.42it/s][A

 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:02<00:00,  2.35it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:03<00:00,  2.44it/s][A
                                               

                                             
[A
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 32/64 [00:58<00:50,  1.57s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:03<00:00,  2.44it/s][A

                                             [ASaving model checkpoint to results/checkpoints/20q/gpt2-large/inversed/checkpoint-32
Configuration saved in results/checkpoints/20q/gpt2-large/inversed/checkpoint-32/config.json
Model weights saved in results/checkpoints/20q/gpt2-large/inversed/checkpoint-32/pytorch_model.bin

 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 33/64 [03:15<22:17, 43.16s/it]
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 34/64 [03:16<15:21, 30.73s/it]
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 35/64 [03:18<10:38, 22.03s/it]
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 36/64 [03:20<07:26, 15.93s/it]
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 37/64 [03:22<05:15, 11.67s/it]
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 38/64 [03:23<03:45,  8.68s/it]
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 39/64 [03:25<02:44,  6.59s/it]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 40/64 [03:27<02:03,  5.13s/it]
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 41/64 [03:28<01:34,  4.11s/it]
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 42/64 [03:30<01:14,  3.39s/it]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 43/64 [03:32<01:00,  2.89s/it]
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 44/64 [03:34<00:50,  2.54s/it]
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 45/64 [03:35<00:43,  2.29s/it]
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 46/64 [03:37<00:38,  2.12s/it]
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 47/64 [03:39<00:34,  2.00s/it]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 48/64 [03:40<00:30,  1.92s/it]
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 49/64 [03:42<00:27,  1.86s/it]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 50/64 [03:44<00:25,  1.82s/it]
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 51/64 [03:46<00:23,  1.79s/it]
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 52/64 [03:47<00:21,  1.78s/it]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 53/64 [03:49<00:19,  1.76s/it]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 54/64 [03:51<00:17,  1.75s/it]
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 55/64 [03:53<00:15,  1.75s/it]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 56/64 [03:54<00:13,  1.74s/it]
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 57/64 [03:56<00:12,  1.74s/it]
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 58/64 [03:58<00:10,  1.74s/it]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 59/64 [03:59<00:08,  1.74s/it]
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 60/64 [04:01<00:06,  1.74s/it]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 61/64 [04:03<00:05,  1.73s/it]
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 62/64 [04:05<00:03,  1.73s/it]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 63/64 [04:06<00:01,  1.73s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [04:08<00:00,  1.58s/it]***** Running Evaluation *****
  Num examples = 496
  Batch size = 64
{'eval_loss': 1.0685625076293945, 'eval_runtime': 3.5276, 'eval_samples_per_second': 140.607, 'eval_steps_per_second': 2.268, 'epoch': 1.0}


  0%|          | 0/8 [00:00<?, ?it/s][A

 25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:01,  4.41it/s][A

 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:00<00:01,  3.11it/s][A

 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:01<00:01,  2.69it/s][A

 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:01<00:01,  2.50it/s][A

 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:02<00:00,  2.39it/s][A

 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:02<00:00,  2.33it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:03<00:00,  2.41it/s][A
                                               

                                             
[A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [04:11<00:00,  1.58s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:03<00:00,  2.41it/s][A

                                             [ASaving model checkpoint to results/checkpoints/20q/gpt2-large/inversed/checkpoint-64
Configuration saved in results/checkpoints/20q/gpt2-large/inversed/checkpoint-64/config.json
Model weights saved in results/checkpoints/20q/gpt2-large/inversed/checkpoint-64/pytorch_model.bin
Deleting older checkpoint [results/checkpoints/20q/gpt2-large/inversed/checkpoint-16] due to args.save_total_limit


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from results/checkpoints/20q/gpt2-large/inversed/checkpoint-64 (score: 1.0386970043182373).

                                               

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [06:12<00:00,  1.58s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 64/64 [06:12<00:00,  5.82s/it]
***** Running Evaluation *****
  Num examples = 496
  Batch size = 64
{'eval_loss': 1.0386970043182373, 'eval_runtime': 3.5642, 'eval_samples_per_second': 139.16, 'eval_steps_per_second': 2.245, 'epoch': 2.0}
{'train_runtime': 372.5908, 'train_samples_per_second': 10.843, 'train_steps_per_second': 0.172, 'train_loss': 1.310753345489502, 'epoch': 2.0}

  0%|          | 0/8 [00:00<?, ?it/s]
 25%|â–ˆâ–ˆâ–Œ       | 2/8 [00:00<00:01,  4.44it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 3/8 [00:00<00:01,  3.13it/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 4/8 [00:01<00:01,  2.72it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 5/8 [00:01<00:01,  2.52it/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 6/8 [00:02<00:00,  2.41it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 7/8 [00:02<00:00,  2.35it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:03<00:00,  2.43it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:03<00:00,  2.57it/s]
Saving model checkpoint to results/best-checkpoints/20q/gpt2-large/inversed
Configuration saved in results/best-checkpoints/20q/gpt2-large/inversed/config.json
Model weights saved in results/best-checkpoints/20q/gpt2-large/inversed/pytorch_model.bin
{'eval_loss': 1.0386970043182373, 'eval_runtime': 3.5248, 'eval_samples_per_second': 140.716, 'eval_steps_per_second': 2.27, 'epoch': 2.0}
Loaded dataset with 2020 elements
Loaded dataset with 496 elements
Loaded dataset with 2520 elements
train Accuracy =  0.22326732673267327
train F1 score =  0.24140714678908495
train Confusion Matrix = 
 [[183   3 808]
 [  0 268 758]
 [  0   0   0]]
Classification Report: 
               precision    recall  f1-score   support

        True       1.00      0.18      0.31       994
       False       0.99      0.26      0.41      1026
        None       0.00      1.00      0.00         0

    accuracy                           0.22      2020
   macro avg       0.66      0.48      0.24      2020
weighted avg       0.99      0.22      0.36      2020

valid Accuracy =  0.23387096774193547
valid F1 score =  0.2497608338687799
valid Confusion Matrix = 
 [[ 42   0 206]
 [  0  74 174]
 [  0   0   0]]
Classification Report: 
               precision    recall  f1-score   support

        True       1.00      0.17      0.29       248
       False       1.00      0.30      0.46       248
        None       0.00      1.00      0.00         0

    accuracy                           0.23       496
   macro avg       0.67      0.49      0.25       496
weighted avg       1.00      0.23      0.37       496

test Accuracy =  0.22341269841269842
test F1 score =  0.24008860660236808
test Confusion Matrix = 
 [[ 210    8 1028]
 [   0  353  921]
 [   0    0    0]]
Classification Report: 
               precision    recall  f1-score   support

        True       1.00      0.17      0.29      1246
       False       0.98      0.28      0.43      1274
        None       0.00      1.00      0.00         0

    accuracy                           0.22      2520
   macro avg       0.66      0.48      0.24      2520
weighted avg       0.99      0.22      0.36      2520

