/work/anshitagupta_umass_edu/miniconda3/envs/memit/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 2040
  Num Epochs = 5
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 160
Loaded dataset with 2040 elements
Loaded dataset with 508 elements

  0%|          | 0/160 [00:00<?, ?it/s]
  1%|          | 1/160 [00:02<05:39,  2.14s/it]
  1%|▏         | 2/160 [00:03<04:15,  1.61s/it]
  2%|▏         | 3/160 [00:04<03:47,  1.45s/it]
  2%|▎         | 4/160 [00:05<03:33,  1.37s/it]
  3%|▎         | 5/160 [00:07<03:25,  1.33s/it]
  4%|▍         | 6/160 [00:08<03:20,  1.30s/it]
  4%|▍         | 7/160 [00:09<03:16,  1.28s/it]
  5%|▌         | 8/160 [00:10<03:13,  1.27s/it]
  6%|▌         | 9/160 [00:12<03:11,  1.27s/it]
  6%|▋         | 10/160 [00:13<03:09,  1.27s/it]
  7%|▋         | 11/160 [00:14<03:08,  1.26s/it]
  8%|▊         | 12/160 [00:15<03:06,  1.26s/it]
  8%|▊         | 13/160 [00:17<03:05,  1.26s/it]
  9%|▉         | 14/160 [00:18<03:03,  1.26s/it]
  9%|▉         | 15/160 [00:19<03:02,  1.26s/it]
 10%|█         | 16/160 [00:20<03:01,  1.26s/it]
 11%|█         | 17/160 [00:22<02:59,  1.26s/it]
 11%|█▏        | 18/160 [00:23<02:58,  1.26s/it]
 12%|█▏        | 19/160 [00:24<02:57,  1.26s/it]
 12%|█▎        | 20/160 [00:25<02:56,  1.26s/it]
 13%|█▎        | 21/160 [00:27<02:55,  1.26s/it]
 14%|█▍        | 22/160 [00:28<02:53,  1.26s/it]
 14%|█▍        | 23/160 [00:29<02:53,  1.26s/it]
 15%|█▌        | 24/160 [00:31<02:51,  1.26s/it]
 16%|█▌        | 25/160 [00:32<02:50,  1.26s/it]
 16%|█▋        | 26/160 [00:33<02:49,  1.26s/it]
 17%|█▋        | 27/160 [00:34<02:48,  1.26s/it]
 18%|█▊        | 28/160 [00:36<02:46,  1.26s/it]
 18%|█▊        | 29/160 [00:37<02:45,  1.26s/it]
 19%|█▉        | 30/160 [00:38<02:44,  1.27s/it]
 19%|█▉        | 31/160 [00:39<02:43,  1.27s/it]
 20%|██        | 32/160 [00:41<02:37,  1.23s/it]***** Running Evaluation *****
  Num examples = 508
  Batch size = 64


  0%|          | 0/8 [00:00<?, ?it/s][A

 25%|██▌       | 2/8 [00:00<00:00,  6.27it/s][A

 38%|███▊      | 3/8 [00:00<00:01,  4.39it/s][A

 50%|█████     | 4/8 [00:00<00:01,  3.81it/s][A

 62%|██████▎   | 5/8 [00:01<00:00,  3.54it/s][A

 75%|███████▌  | 6/8 [00:01<00:00,  3.39it/s][A

 88%|████████▊ | 7/8 [00:01<00:00,  3.29it/s][A

100%|██████████| 8/8 [00:02<00:00,  3.30it/s][A
                                                

                                             
[A
 20%|██        | 32/160 [00:43<02:37,  1.23s/it]

100%|██████████| 8/8 [00:02<00:00,  3.30it/s][A

                                             [ASaving model checkpoint to results/checkpoints/20q/gpt2-xl/normal/checkpoint-32
Configuration saved in results/checkpoints/20q/gpt2-xl/normal/checkpoint-32/config.json
Model weights saved in results/checkpoints/20q/gpt2-xl/normal/checkpoint-32/pytorch_model.bin

 21%|██        | 33/160 [03:57<2:06:51, 59.93s/it]
 21%|██▏       | 34/160 [03:59<1:28:53, 42.33s/it]
 22%|██▏       | 35/160 [04:00<1:02:30, 30.00s/it]
 22%|██▎       | 36/160 [04:01<44:10, 21.38s/it]  
 23%|██▎       | 37/160 [04:02<31:26, 15.34s/it]
 24%|██▍       | 38/160 [04:04<22:35, 11.11s/it]
 24%|██▍       | 39/160 [04:05<16:26,  8.15s/it]
 25%|██▌       | 40/160 [04:06<12:09,  6.08s/it]
 26%|██▌       | 41/160 [04:07<09:11,  4.63s/it]
 26%|██▋       | 42/160 [04:09<07:07,  3.62s/it]
 27%|██▋       | 43/160 [04:10<05:40,  2.91s/it]
 28%|██▊       | 44/160 [04:11<04:39,  2.41s/it]
 28%|██▊       | 45/160 [04:12<03:57,  2.07s/it]
 29%|██▉       | 46/160 [04:14<03:27,  1.82s/it]
 29%|██▉       | 47/160 [04:15<03:06,  1.65s/it]
 30%|███       | 48/160 [04:16<02:51,  1.53s/it]
 31%|███       | 49/160 [04:17<02:41,  1.45s/it]
 31%|███▏      | 50/160 [04:19<02:33,  1.39s/it]
 32%|███▏      | 51/160 [04:20<02:27,  1.35s/it]
 32%|███▎      | 52/160 [04:21<02:23,  1.33s/it]
 33%|███▎      | 53/160 [04:23<02:19,  1.31s/it]
 34%|███▍      | 54/160 [04:24<02:17,  1.29s/it]
 34%|███▍      | 55/160 [04:25<02:14,  1.28s/it]
 35%|███▌      | 56/160 [04:26<02:12,  1.28s/it]
 36%|███▌      | 57/160 [04:28<02:11,  1.27s/it]
 36%|███▋      | 58/160 [04:29<02:10,  1.28s/it]
 37%|███▋      | 59/160 [04:30<02:08,  1.27s/it]
 38%|███▊      | 60/160 [04:31<02:07,  1.27s/it]
 38%|███▊      | 61/160 [04:33<02:05,  1.27s/it]
 39%|███▉      | 62/160 [04:34<02:04,  1.27s/it]
 39%|███▉      | 63/160 [04:35<02:03,  1.27s/it]
 40%|████      | 64/160 [04:36<01:58,  1.23s/it]***** Running Evaluation *****
  Num examples = 508
  Batch size = 64
{'eval_loss': 1.0344264507293701, 'eval_runtime': 2.5487, 'eval_samples_per_second': 199.318, 'eval_steps_per_second': 3.139, 'epoch': 1.0}


  0%|          | 0/8 [00:00<?, ?it/s][A

 25%|██▌       | 2/8 [00:00<00:00,  6.24it/s][A

 38%|███▊      | 3/8 [00:00<00:01,  4.39it/s][A

 50%|█████     | 4/8 [00:00<00:01,  3.81it/s][A

 62%|██████▎   | 5/8 [00:01<00:00,  3.54it/s][A

 75%|███████▌  | 6/8 [00:01<00:00,  3.38it/s][A

 88%|████████▊ | 7/8 [00:01<00:00,  3.28it/s][A

100%|██████████| 8/8 [00:02<00:00,  3.29it/s][A
                                                

                                             
[A
 40%|████      | 64/160 [04:39<01:58,  1.23s/it]

100%|██████████| 8/8 [00:02<00:00,  3.29it/s][A

                                             [ASaving model checkpoint to results/checkpoints/20q/gpt2-xl/normal/checkpoint-64
Configuration saved in results/checkpoints/20q/gpt2-xl/normal/checkpoint-64/config.json
Model weights saved in results/checkpoints/20q/gpt2-xl/normal/checkpoint-64/pytorch_model.bin
Deleting older checkpoint [results/checkpoints/20q/gpt2-xl/normal/checkpoint-96] due to args.save_total_limit

 41%|████      | 65/160 [07:47<1:31:54, 58.05s/it]
 41%|████▏     | 66/160 [07:48<1:04:14, 41.01s/it]
 42%|████▏     | 67/160 [07:49<45:04, 29.08s/it]  
 42%|████▎     | 68/160 [07:51<31:47, 20.73s/it]
 43%|████▎     | 69/160 [07:52<22:34, 14.89s/it]
 44%|████▍     | 70/160 [07:53<16:11, 10.79s/it]
 44%|████▍     | 71/160 [07:54<11:45,  7.93s/it]
 45%|████▌     | 72/160 [07:56<08:41,  5.93s/it]
 46%|████▌     | 73/160 [07:57<06:33,  4.52s/it]
 46%|████▋     | 74/160 [07:58<05:04,  3.54s/it]
 47%|████▋     | 75/160 [07:59<04:02,  2.86s/it]
 48%|████▊     | 76/160 [08:01<03:19,  2.38s/it]
 48%|████▊     | 77/160 [08:02<02:49,  2.04s/it]
 49%|████▉     | 78/160 [08:03<02:28,  1.81s/it]
 49%|████▉     | 79/160 [08:05<02:12,  1.64s/it]
 50%|█████     | 80/160 [08:06<02:02,  1.53s/it]
 51%|█████     | 81/160 [08:07<01:54,  1.45s/it]
 51%|█████▏    | 82/160 [08:08<01:48,  1.39s/it]
 52%|█████▏    | 83/160 [08:10<01:43,  1.35s/it]
 52%|█████▎    | 84/160 [08:11<01:40,  1.33s/it]
 53%|█████▎    | 85/160 [08:12<01:37,  1.31s/it]
 54%|█████▍    | 86/160 [08:13<01:35,  1.29s/it]
 54%|█████▍    | 87/160 [08:15<01:33,  1.28s/it]
 55%|█████▌    | 88/160 [08:16<01:32,  1.28s/it]
 56%|█████▌    | 89/160 [08:17<01:30,  1.28s/it]
 56%|█████▋    | 90/160 [08:18<01:29,  1.27s/it]
 57%|█████▋    | 91/160 [08:20<01:27,  1.27s/it]
 57%|█████▊    | 92/160 [08:21<01:26,  1.27s/it]
 58%|█████▊    | 93/160 [08:22<01:24,  1.27s/it]
 59%|█████▉    | 94/160 [08:23<01:23,  1.27s/it]
 59%|█████▉    | 95/160 [08:25<01:22,  1.27s/it]
 60%|██████    | 96/160 [08:26<01:18,  1.23s/it]***** Running Evaluation *****
  Num examples = 508
  Batch size = 64
{'eval_loss': 1.0488345623016357, 'eval_runtime': 2.5528, 'eval_samples_per_second': 198.995, 'eval_steps_per_second': 3.134, 'epoch': 2.0}


  0%|          | 0/8 [00:00<?, ?it/s][A

 25%|██▌       | 2/8 [00:00<00:00,  6.29it/s][A

 38%|███▊      | 3/8 [00:00<00:01,  4.39it/s][A

 50%|█████     | 4/8 [00:00<00:01,  3.82it/s][A

 62%|██████▎   | 5/8 [00:01<00:00,  3.55it/s][A

 75%|███████▌  | 6/8 [00:01<00:00,  3.39it/s][A

 88%|████████▊ | 7/8 [00:01<00:00,  3.30it/s][A

100%|██████████| 8/8 [00:02<00:00,  3.30it/s][A
                                                

                                             
[A
 60%|██████    | 96/160 [08:28<01:18,  1.23s/it]

100%|██████████| 8/8 [00:02<00:00,  3.30it/s][A

                                             [ASaving model checkpoint to results/checkpoints/20q/gpt2-xl/normal/checkpoint-96
Configuration saved in results/checkpoints/20q/gpt2-xl/normal/checkpoint-96/config.json
Model weights saved in results/checkpoints/20q/gpt2-xl/normal/checkpoint-96/pytorch_model.bin
Deleting older checkpoint [results/checkpoints/20q/gpt2-xl/normal/checkpoint-64] due to args.save_total_limit

 61%|██████    | 97/160 [11:57<1:07:18, 64.11s/it]
 61%|██████▏   | 98/160 [11:58<46:45, 45.25s/it]  
 62%|██████▏   | 99/160 [11:59<32:34, 32.05s/it]
 62%|██████▎   | 100/160 [12:00<22:48, 22.81s/it]
 63%|██████▎   | 101/160 [12:02<16:04, 16.34s/it]
 64%|██████▍   | 102/160 [12:03<11:25, 11.81s/it]
 64%|██████▍   | 103/160 [12:04<08:12,  8.64s/it]
 65%|██████▌   | 104/160 [12:05<05:59,  6.43s/it]
 66%|██████▌   | 105/160 [12:07<04:28,  4.87s/it]
 66%|██████▋   | 106/160 [12:08<03:24,  3.79s/it]
 67%|██████▋   | 107/160 [12:09<02:40,  3.03s/it]
 68%|██████▊   | 108/160 [12:10<02:09,  2.50s/it]
 68%|██████▊   | 109/160 [12:12<01:48,  2.12s/it]
 69%|██████▉   | 110/160 [12:13<01:33,  1.86s/it]
 69%|██████▉   | 111/160 [12:14<01:22,  1.68s/it]
 70%|███████   | 112/160 [12:15<01:14,  1.55s/it]
 71%|███████   | 113/160 [12:17<01:08,  1.47s/it]
 71%|███████▏  | 114/160 [12:18<01:04,  1.40s/it]
 72%|███████▏  | 115/160 [12:19<01:01,  1.36s/it]
 72%|███████▎  | 116/160 [12:21<00:58,  1.33s/it]
 73%|███████▎  | 117/160 [12:22<00:56,  1.31s/it]
 74%|███████▍  | 118/160 [12:23<00:54,  1.29s/it]
 74%|███████▍  | 119/160 [12:24<00:52,  1.28s/it]
 75%|███████▌  | 120/160 [12:26<00:51,  1.28s/it]
 76%|███████▌  | 121/160 [12:27<00:49,  1.28s/it]
 76%|███████▋  | 122/160 [12:28<00:48,  1.27s/it]
 77%|███████▋  | 123/160 [12:29<00:46,  1.27s/it]
 78%|███████▊  | 124/160 [12:31<00:45,  1.27s/it]
 78%|███████▊  | 125/160 [12:32<00:44,  1.27s/it]
 79%|███████▉  | 126/160 [12:33<00:43,  1.27s/it]
 79%|███████▉  | 127/160 [12:34<00:41,  1.27s/it]
 80%|████████  | 128/160 [12:36<00:39,  1.23s/it]***** Running Evaluation *****
  Num examples = 508
  Batch size = 64
{'eval_loss': 1.1420751810073853, 'eval_runtime': 2.5457, 'eval_samples_per_second': 199.553, 'eval_steps_per_second': 3.143, 'epoch': 3.0}


  0%|          | 0/8 [00:00<?, ?it/s][A

 25%|██▌       | 2/8 [00:00<00:00,  6.25it/s][A

 38%|███▊      | 3/8 [00:00<00:01,  4.37it/s][A

 50%|█████     | 4/8 [00:00<00:01,  3.79it/s][A

 62%|██████▎   | 5/8 [00:01<00:00,  3.52it/s][A

 75%|███████▌  | 6/8 [00:01<00:00,  3.37it/s][A

 88%|████████▊ | 7/8 [00:01<00:00,  3.28it/s][A

100%|██████████| 8/8 [00:02<00:00,  3.29it/s][A
                                                 

                                             
[A
 80%|████████  | 128/160 [12:38<00:39,  1.23s/it]

100%|██████████| 8/8 [00:02<00:00,  3.29it/s][A

                                             [ASaving model checkpoint to results/checkpoints/20q/gpt2-xl/normal/checkpoint-128
Configuration saved in results/checkpoints/20q/gpt2-xl/normal/checkpoint-128/config.json
Model weights saved in results/checkpoints/20q/gpt2-xl/normal/checkpoint-128/pytorch_model.bin
Deleting older checkpoint [results/checkpoints/20q/gpt2-xl/normal/checkpoint-96] due to args.save_total_limit

 81%|████████  | 129/160 [16:31<36:58, 71.56s/it]
 81%|████████▏ | 130/160 [16:32<25:13, 50.46s/it]
 82%|████████▏ | 131/160 [16:34<17:15, 35.70s/it]
 82%|████████▎ | 132/160 [16:35<11:50, 25.36s/it]
 83%|████████▎ | 133/160 [16:36<08:09, 18.13s/it]
 84%|████████▍ | 134/160 [16:37<05:39, 13.06s/it]
 84%|████████▍ | 135/160 [16:39<03:57,  9.52s/it]
 85%|████████▌ | 136/160 [16:40<02:48,  7.04s/it]
 86%|████████▌ | 137/160 [16:41<02:01,  5.30s/it]
 86%|████████▋ | 138/160 [16:42<01:29,  4.09s/it]
 87%|████████▋ | 139/160 [16:44<01:08,  3.24s/it]
 88%|████████▊ | 140/160 [16:45<00:52,  2.64s/it]
 88%|████████▊ | 141/160 [16:46<00:42,  2.23s/it]
 89%|████████▉ | 142/160 [16:47<00:34,  1.94s/it]
 89%|████████▉ | 143/160 [16:49<00:29,  1.73s/it]
 90%|█████████ | 144/160 [16:50<00:25,  1.59s/it]
 91%|█████████ | 145/160 [16:51<00:22,  1.49s/it]
 91%|█████████▏| 146/160 [16:53<00:19,  1.42s/it]
 92%|█████████▏| 147/160 [16:54<00:17,  1.37s/it]
 92%|█████████▎| 148/160 [16:55<00:16,  1.34s/it]
 93%|█████████▎| 149/160 [16:56<00:14,  1.32s/it]
 94%|█████████▍| 150/160 [16:58<00:12,  1.30s/it]
 94%|█████████▍| 151/160 [16:59<00:11,  1.29s/it]
 95%|█████████▌| 152/160 [17:00<00:10,  1.28s/it]
 96%|█████████▌| 153/160 [17:01<00:08,  1.28s/it]
 96%|█████████▋| 154/160 [17:03<00:07,  1.27s/it]
 97%|█████████▋| 155/160 [17:04<00:06,  1.27s/it]
 98%|█████████▊| 156/160 [17:05<00:05,  1.27s/it]
 98%|█████████▊| 157/160 [17:06<00:03,  1.27s/it]
 99%|█████████▉| 158/160 [17:08<00:02,  1.27s/it]
 99%|█████████▉| 159/160 [17:09<00:01,  1.27s/it]
100%|██████████| 160/160 [17:10<00:00,  1.24s/it]***** Running Evaluation *****
  Num examples = 508
  Batch size = 64
{'eval_loss': 1.2414945363998413, 'eval_runtime': 2.5565, 'eval_samples_per_second': 198.707, 'eval_steps_per_second': 3.129, 'epoch': 4.0}


  0%|          | 0/8 [00:00<?, ?it/s][A

 25%|██▌       | 2/8 [00:00<00:00,  6.25it/s][A

 38%|███▊      | 3/8 [00:00<00:01,  4.38it/s][A

 50%|█████     | 4/8 [00:00<00:01,  3.81it/s][A

 62%|██████▎   | 5/8 [00:01<00:00,  3.53it/s][A

 75%|███████▌  | 6/8 [00:01<00:00,  3.38it/s][A

 88%|████████▊ | 7/8 [00:01<00:00,  3.29it/s][A

100%|██████████| 8/8 [00:02<00:00,  3.30it/s][A
                                                 

                                             
[A
100%|██████████| 160/160 [17:13<00:00,  1.24s/it]

100%|██████████| 8/8 [00:02<00:00,  3.30it/s][A

                                             [ASaving model checkpoint to results/checkpoints/20q/gpt2-xl/normal/checkpoint-160
Configuration saved in results/checkpoints/20q/gpt2-xl/normal/checkpoint-160/config.json
Model weights saved in results/checkpoints/20q/gpt2-xl/normal/checkpoint-160/pytorch_model.bin
Deleting older checkpoint [results/checkpoints/20q/gpt2-xl/normal/checkpoint-128] due to args.save_total_limit


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from results/checkpoints/20q/gpt2-xl/normal/checkpoint-32 (score: 1.0344264507293701).

                                                 

100%|██████████| 160/160 [22:43<00:00,  1.24s/it]
100%|██████████| 160/160 [22:43<00:00,  8.52s/it]
***** Running Evaluation *****
  Num examples = 508
  Batch size = 64
{'eval_loss': 1.3082066774368286, 'eval_runtime': 2.5532, 'eval_samples_per_second': 198.968, 'eval_steps_per_second': 3.133, 'epoch': 5.0}
{'train_runtime': 1363.1871, 'train_samples_per_second': 7.482, 'train_steps_per_second': 0.117, 'train_loss': 0.7174863815307617, 'epoch': 5.0}

  0%|          | 0/8 [00:00<?, ?it/s]
 25%|██▌       | 2/8 [00:00<00:00,  6.45it/s]
 38%|███▊      | 3/8 [00:00<00:01,  4.54it/s]
 50%|█████     | 4/8 [00:00<00:01,  3.92it/s]
 62%|██████▎   | 5/8 [00:01<00:00,  3.64it/s]
 75%|███████▌  | 6/8 [00:01<00:00,  3.48it/s]
 88%|████████▊ | 7/8 [00:01<00:00,  3.39it/s]
100%|██████████| 8/8 [00:02<00:00,  3.39it/s]
100%|██████████| 8/8 [00:02<00:00,  3.66it/s]
Saving model checkpoint to results/best-checkpoints/20q/gpt2-xl/normal
Configuration saved in results/best-checkpoints/20q/gpt2-xl/normal/config.json
Model weights saved in results/best-checkpoints/20q/gpt2-xl/normal/pytorch_model.bin
loading configuration file results/best-checkpoints/20q/gpt2-xl/normal/config.json
Model config GPT2Config {
  "_name_or_path": "results/best-checkpoints/20q/gpt2-xl/normal",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1600,
  "n_head": 25,
  "n_inner": null,
  "n_layer": 48,
  "n_positions": 1024,
  "output_past": true,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.23.1",
  "use_cache": true,
  "vocab_size": 50257
}

loading weights file results/best-checkpoints/20q/gpt2-xl/normal/pytorch_model.bin
All model checkpoint weights were used when initializing GPT2LMHeadModel.

All the weights of GPT2LMHeadModel were initialized from the model checkpoint at results/best-checkpoints/20q/gpt2-xl/normal.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /work/anshitagupta_umass_edu/.cache/huggingface/hub/models--gpt2-xl/snapshots/33cdb5c0db5423c1879b1b9f16c352988e8754a8/config.json
Model config GPT2Config {
  "_name_or_path": "gpt2-xl",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1600,
  "n_head": 25,
  "n_inner": null,
  "n_layer": 48,
  "n_positions": 1024,
  "output_past": true,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.23.1",
  "use_cache": true,
  "vocab_size": 50257
}

loading file vocab.json from cache at /work/anshitagupta_umass_edu/.cache/huggingface/hub/models--gpt2-xl/snapshots/33cdb5c0db5423c1879b1b9f16c352988e8754a8/vocab.json
loading file merges.txt from cache at /work/anshitagupta_umass_edu/.cache/huggingface/hub/models--gpt2-xl/snapshots/33cdb5c0db5423c1879b1b9f16c352988e8754a8/merges.txt
loading file tokenizer.json from cache at /work/anshitagupta_umass_edu/.cache/huggingface/hub/models--gpt2-xl/snapshots/33cdb5c0db5423c1879b1b9f16c352988e8754a8/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /work/anshitagupta_umass_edu/.cache/huggingface/hub/models--gpt2-xl/snapshots/33cdb5c0db5423c1879b1b9f16c352988e8754a8/config.json
Model config GPT2Config {
  "_name_or_path": "gpt2-xl",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1600,
  "n_head": 25,
  "n_inner": null,
  "n_layer": 48,
  "n_positions": 1024,
  "output_past": true,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.23.1",
  "use_cache": true,
  "vocab_size": 50257
}

{'eval_loss': 1.0344264507293701, 'eval_runtime': 2.5131, 'eval_samples_per_second': 202.137, 'eval_steps_per_second': 3.183, 'epoch': 5.0}
Loaded dataset with 2040 elements
Loaded dataset with 508 elements
Loaded dataset with 2535 elements
train Accuracy =  0.17009803921568628
train F1 score =  0.19201989050658633
train Confusion Matrix = 
 [[126   0 894]
 [  0 221 799]
 [  0   0   0]]
Classification Report: 
               precision    recall  f1-score   support

        True       1.00      0.12      0.22      1020
       False       1.00      0.22      0.36      1020
        None       0.00      1.00      0.00         0

    accuracy                           0.17      2040
   macro avg       0.67      0.45      0.19      2040
weighted avg       1.00      0.17      0.29      2040

valid Accuracy =  0.14763779527559054
valid F1 score =  0.17001720440244159
valid Confusion Matrix = 
 [[ 48   0 206]
 [  0  27 227]
 [  0   0   0]]
Classification Report: 
               precision    recall  f1-score   support

        True       1.00      0.19      0.32       254
       False       1.00      0.11      0.19       254
        None       0.00      1.00      0.00         0

    accuracy                           0.15       508
   macro avg       0.67      0.43      0.17       508
weighted avg       1.00      0.15      0.26       508

test Accuracy =  0.1581854043392505
test F1 score =  0.18086746212894408
test Confusion Matrix = 
 [[ 157    0 1104]
 [   2  244 1028]
 [   0    0    0]]
Classification Report: 
               precision    recall  f1-score   support

        True       0.99      0.12      0.22      1261
       False       1.00      0.19      0.32      1274
        None       0.00      1.00      0.00         0

    accuracy                           0.16      2535
   macro avg       0.66      0.44      0.18      2535
weighted avg       0.99      0.16      0.27      2535