/work/anshitagupta_umass_edu/miniconda3/envs/memit/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 4060
  Num Epochs = 5
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 320
Loaded dataset with 4060 elements
Loaded dataset with 1004 elements

  0%|          | 0/320 [00:00<?, ?it/s]
  0%|          | 1/320 [00:02<11:46,  2.22s/it]
  1%|          | 2/320 [00:03<08:44,  1.65s/it]
  1%|          | 3/320 [00:04<07:44,  1.47s/it]
  1%|▏         | 4/320 [00:05<07:16,  1.38s/it]
  2%|▏         | 5/320 [00:07<06:59,  1.33s/it]
  2%|▏         | 6/320 [00:08<06:49,  1.30s/it]
  2%|▏         | 7/320 [00:09<06:42,  1.29s/it]
  2%|▎         | 8/320 [00:10<06:38,  1.28s/it]
  3%|▎         | 9/320 [00:12<06:34,  1.27s/it]
  3%|▎         | 10/320 [00:13<06:32,  1.26s/it]
  3%|▎         | 11/320 [00:14<06:29,  1.26s/it]
  4%|▍         | 12/320 [00:15<06:28,  1.26s/it]
  4%|▍         | 13/320 [00:17<06:26,  1.26s/it]
  4%|▍         | 14/320 [00:18<06:25,  1.26s/it]
  5%|▍         | 15/320 [00:19<06:24,  1.26s/it]
  5%|▌         | 16/320 [00:21<06:23,  1.26s/it]
  5%|▌         | 17/320 [00:22<06:21,  1.26s/it]
  6%|▌         | 18/320 [00:23<06:20,  1.26s/it]
  6%|▌         | 19/320 [00:24<06:19,  1.26s/it]
  6%|▋         | 20/320 [00:26<06:18,  1.26s/it]
  7%|▋         | 21/320 [00:27<06:17,  1.26s/it]
  7%|▋         | 22/320 [00:28<06:16,  1.26s/it]
  7%|▋         | 23/320 [00:29<06:14,  1.26s/it]
  8%|▊         | 24/320 [00:31<06:13,  1.26s/it]
  8%|▊         | 25/320 [00:32<06:12,  1.26s/it]
  8%|▊         | 26/320 [00:33<06:11,  1.26s/it]
  8%|▊         | 27/320 [00:34<06:10,  1.27s/it]
  9%|▉         | 28/320 [00:36<06:09,  1.27s/it]
  9%|▉         | 29/320 [00:37<06:08,  1.27s/it]
  9%|▉         | 30/320 [00:38<06:07,  1.27s/it]
 10%|▉         | 31/320 [00:39<06:06,  1.27s/it]
 10%|█         | 32/320 [00:41<06:05,  1.27s/it]
 10%|█         | 33/320 [00:42<06:04,  1.27s/it]
 11%|█         | 34/320 [00:43<06:03,  1.27s/it]
 11%|█         | 35/320 [00:45<06:01,  1.27s/it]
 11%|█▏        | 36/320 [00:46<06:00,  1.27s/it]
 12%|█▏        | 37/320 [00:47<05:59,  1.27s/it]
 12%|█▏        | 38/320 [00:48<05:58,  1.27s/it]
 12%|█▏        | 39/320 [00:50<05:57,  1.27s/it]
 12%|█▎        | 40/320 [00:51<05:56,  1.27s/it]
 13%|█▎        | 41/320 [00:52<05:54,  1.27s/it]
 13%|█▎        | 42/320 [00:53<05:53,  1.27s/it]
 13%|█▎        | 43/320 [00:55<05:52,  1.27s/it]
 14%|█▍        | 44/320 [00:56<05:51,  1.27s/it]
 14%|█▍        | 45/320 [00:57<05:50,  1.27s/it]
 14%|█▍        | 46/320 [00:59<05:49,  1.27s/it]
 15%|█▍        | 47/320 [01:00<05:47,  1.27s/it]
 15%|█▌        | 48/320 [01:01<05:46,  1.28s/it]
 15%|█▌        | 49/320 [01:02<05:45,  1.28s/it]
 16%|█▌        | 50/320 [01:04<05:44,  1.28s/it]
 16%|█▌        | 51/320 [01:05<05:43,  1.28s/it]
 16%|█▋        | 52/320 [01:06<05:41,  1.28s/it]
 17%|█▋        | 53/320 [01:08<05:40,  1.28s/it]
 17%|█▋        | 54/320 [01:09<05:39,  1.28s/it]
 17%|█▋        | 55/320 [01:10<05:38,  1.28s/it]
 18%|█▊        | 56/320 [01:11<05:37,  1.28s/it]
 18%|█▊        | 57/320 [01:13<05:35,  1.28s/it]
 18%|█▊        | 58/320 [01:14<05:34,  1.28s/it]
 18%|█▊        | 59/320 [01:15<05:33,  1.28s/it]
 19%|█▉        | 60/320 [01:16<05:32,  1.28s/it]
 19%|█▉        | 61/320 [01:18<05:31,  1.28s/it]
 19%|█▉        | 62/320 [01:19<05:29,  1.28s/it]
 20%|█▉        | 63/320 [01:20<05:28,  1.28s/it]
 20%|██        | 64/320 [01:21<04:51,  1.14s/it]***** Running Evaluation *****
  Num examples = 1004
  Batch size = 64


  0%|          | 0/16 [00:00<?, ?it/s][A

 12%|█▎        | 2/16 [00:00<00:02,  6.21it/s][A

 19%|█▉        | 3/16 [00:00<00:02,  4.35it/s][A

 25%|██▌       | 4/16 [00:00<00:03,  3.78it/s][A

 31%|███▏      | 5/16 [00:01<00:03,  3.51it/s][A

 38%|███▊      | 6/16 [00:01<00:02,  3.35it/s][A

 44%|████▍     | 7/16 [00:01<00:02,  3.26it/s][A

 50%|█████     | 8/16 [00:02<00:02,  3.20it/s][A

 56%|█████▋    | 9/16 [00:02<00:02,  3.17it/s][A

 62%|██████▎   | 10/16 [00:02<00:01,  3.14it/s][A

 69%|██████▉   | 11/16 [00:03<00:01,  3.12it/s][A

 75%|███████▌  | 12/16 [00:03<00:01,  3.12it/s][A

 81%|████████▏ | 13/16 [00:03<00:00,  3.11it/s][A

 88%|████████▊ | 14/16 [00:04<00:00,  3.10it/s][A

 94%|█████████▍| 15/16 [00:04<00:00,  3.10it/s][A

100%|██████████| 16/16 [00:04<00:00,  3.38it/s][A
                                                

                                               
[A
 20%|██        | 64/320 [01:26<04:51,  1.14s/it]

100%|██████████| 16/16 [00:04<00:00,  3.38it/s][A

                                               [ASaving model checkpoint to results/checkpoints/20q/gpt2-xl/combined/checkpoint-64
Configuration saved in results/checkpoints/20q/gpt2-xl/combined/checkpoint-64/config.json
Model weights saved in results/checkpoints/20q/gpt2-xl/combined/checkpoint-64/pytorch_model.bin

 20%|██        | 65/320 [05:12<4:58:21, 70.20s/it]
 21%|██        | 66/320 [05:14<3:29:36, 49.51s/it]
 21%|██        | 67/320 [05:15<2:27:43, 35.03s/it]
 21%|██▏       | 68/320 [05:16<1:44:34, 24.90s/it]
 22%|██▏       | 69/320 [05:17<1:14:28, 17.80s/it]
 22%|██▏       | 70/320 [05:19<53:29, 12.84s/it]  
 22%|██▏       | 71/320 [05:20<38:50,  9.36s/it]
 22%|██▎       | 72/320 [05:21<28:38,  6.93s/it]
 23%|██▎       | 73/320 [05:22<21:30,  5.23s/it]
 23%|██▎       | 74/320 [05:24<16:33,  4.04s/it]
 23%|██▎       | 75/320 [05:25<13:04,  3.20s/it]
 24%|██▍       | 76/320 [05:26<10:38,  2.62s/it]
 24%|██▍       | 77/320 [05:27<08:56,  2.21s/it]
 24%|██▍       | 78/320 [05:29<07:45,  1.92s/it]
 25%|██▍       | 79/320 [05:30<06:54,  1.72s/it]
 25%|██▌       | 80/320 [05:31<06:19,  1.58s/it]
 25%|██▌       | 81/320 [05:32<05:54,  1.49s/it]
 26%|██▌       | 82/320 [05:34<05:37,  1.42s/it]
 26%|██▌       | 83/320 [05:35<05:25,  1.37s/it]
 26%|██▋       | 84/320 [05:36<05:16,  1.34s/it]
 27%|██▋       | 85/320 [05:38<05:09,  1.32s/it]
 27%|██▋       | 86/320 [05:39<05:04,  1.30s/it]
 27%|██▋       | 87/320 [05:40<05:00,  1.29s/it]
 28%|██▊       | 88/320 [05:41<04:57,  1.28s/it]
 28%|██▊       | 89/320 [05:43<04:54,  1.28s/it]
 28%|██▊       | 90/320 [05:44<04:52,  1.27s/it]
 28%|██▊       | 91/320 [05:45<04:50,  1.27s/it]
 29%|██▉       | 92/320 [05:46<04:49,  1.27s/it]
 29%|██▉       | 93/320 [05:48<04:47,  1.27s/it]
 29%|██▉       | 94/320 [05:49<04:46,  1.27s/it]
 30%|██▉       | 95/320 [05:50<04:46,  1.27s/it]
 30%|███       | 96/320 [05:51<04:44,  1.27s/it]
 30%|███       | 97/320 [05:53<04:43,  1.27s/it]
 31%|███       | 98/320 [05:54<04:42,  1.27s/it]
 31%|███       | 99/320 [05:55<04:40,  1.27s/it]
 31%|███▏      | 100/320 [05:57<04:39,  1.27s/it]
 32%|███▏      | 101/320 [05:58<04:38,  1.27s/it]
 32%|███▏      | 102/320 [05:59<04:37,  1.27s/it]
 32%|███▏      | 103/320 [06:00<04:36,  1.27s/it]
 32%|███▎      | 104/320 [06:02<04:35,  1.27s/it]
 33%|███▎      | 105/320 [06:03<04:33,  1.27s/it]
 33%|███▎      | 106/320 [06:04<04:32,  1.27s/it]
 33%|███▎      | 107/320 [06:05<04:31,  1.27s/it]
 34%|███▍      | 108/320 [06:07<04:30,  1.27s/it]
 34%|███▍      | 109/320 [06:08<04:28,  1.27s/it]
 34%|███▍      | 110/320 [06:09<04:27,  1.27s/it]
 35%|███▍      | 111/320 [06:11<04:26,  1.27s/it]
 35%|███▌      | 112/320 [06:12<04:25,  1.27s/it]
 35%|███▌      | 113/320 [06:13<04:23,  1.28s/it]
 36%|███▌      | 114/320 [06:14<04:22,  1.28s/it]
 36%|███▌      | 115/320 [06:16<04:22,  1.28s/it]
 36%|███▋      | 116/320 [06:17<04:20,  1.28s/it]
 37%|███▋      | 117/320 [06:18<04:19,  1.28s/it]
 37%|███▋      | 118/320 [06:20<04:18,  1.28s/it]
 37%|███▋      | 119/320 [06:21<04:16,  1.28s/it]
 38%|███▊      | 120/320 [06:22<04:15,  1.28s/it]
 38%|███▊      | 121/320 [06:23<04:14,  1.28s/it]
 38%|███▊      | 122/320 [06:25<04:13,  1.28s/it]
 38%|███▊      | 123/320 [06:26<04:11,  1.28s/it]
 39%|███▉      | 124/320 [06:27<04:10,  1.28s/it]
 39%|███▉      | 125/320 [06:28<04:09,  1.28s/it]
 39%|███▉      | 126/320 [06:30<04:08,  1.28s/it]
 40%|███▉      | 127/320 [06:31<04:06,  1.28s/it]
 40%|████      | 128/320 [06:32<03:36,  1.13s/it]***** Running Evaluation *****
  Num examples = 1004
  Batch size = 64
{'eval_loss': 1.0134471654891968, 'eval_runtime': 5.0875, 'eval_samples_per_second': 197.345, 'eval_steps_per_second': 3.145, 'epoch': 1.0}


  0%|          | 0/16 [00:00<?, ?it/s][A

 12%|█▎        | 2/16 [00:00<00:02,  6.20it/s][A

 19%|█▉        | 3/16 [00:00<00:03,  4.33it/s][A

 25%|██▌       | 4/16 [00:00<00:03,  3.77it/s][A

 31%|███▏      | 5/16 [00:01<00:03,  3.50it/s][A

 38%|███▊      | 6/16 [00:01<00:02,  3.34it/s][A

 44%|████▍     | 7/16 [00:01<00:02,  3.26it/s][A

 50%|█████     | 8/16 [00:02<00:02,  3.20it/s][A

 56%|█████▋    | 9/16 [00:02<00:02,  3.16it/s][A

 62%|██████▎   | 10/16 [00:02<00:01,  3.14it/s][A

 69%|██████▉   | 11/16 [00:03<00:01,  3.12it/s][A

 75%|███████▌  | 12/16 [00:03<00:01,  3.10it/s][A

 81%|████████▏ | 13/16 [00:03<00:00,  3.10it/s][A

 88%|████████▊ | 14/16 [00:04<00:00,  3.09it/s][A

 94%|█████████▍| 15/16 [00:04<00:00,  3.09it/s][A

100%|██████████| 16/16 [00:04<00:00,  3.37it/s][A
                                                 

                                               
[A
 40%|████      | 128/320 [06:37<03:36,  1.13s/it]

100%|██████████| 16/16 [00:04<00:00,  3.37it/s][A

                                               [ASaving model checkpoint to results/checkpoints/20q/gpt2-xl/combined/checkpoint-128
Configuration saved in results/checkpoints/20q/gpt2-xl/combined/checkpoint-128/config.json
Model weights saved in results/checkpoints/20q/gpt2-xl/combined/checkpoint-128/pytorch_model.bin

 40%|████      | 129/320 [10:14<3:35:03, 67.56s/it]
 41%|████      | 130/320 [10:16<2:30:55, 47.66s/it]
 41%|████      | 131/320 [10:17<1:46:16, 33.74s/it]
 41%|████▏     | 132/320 [10:18<1:15:10, 23.99s/it]
 42%|████▏     | 133/320 [10:19<53:30, 17.17s/it]  
 42%|████▏     | 134/320 [10:21<38:25, 12.40s/it]
 42%|████▏     | 135/320 [10:22<27:54,  9.05s/it]
 42%|████▎     | 136/320 [10:23<20:35,  6.71s/it]
 43%|████▎     | 137/320 [10:24<15:29,  5.08s/it]
 43%|████▎     | 138/320 [10:26<11:55,  3.93s/it]
 43%|████▎     | 139/320 [10:27<09:26,  3.13s/it]
 44%|████▍     | 140/320 [10:28<07:42,  2.57s/it]
 44%|████▍     | 141/320 [10:29<06:29,  2.17s/it]
 44%|████▍     | 142/320 [10:31<05:38,  1.90s/it]
 45%|████▍     | 143/320 [10:32<05:02,  1.71s/it]
 45%|████▌     | 144/320 [10:33<04:36,  1.57s/it]
 45%|████▌     | 145/320 [10:34<04:19,  1.48s/it]
 46%|████▌     | 146/320 [10:36<04:06,  1.41s/it]
 46%|████▌     | 147/320 [10:37<03:56,  1.37s/it]
 46%|████▋     | 148/320 [10:38<03:50,  1.34s/it]
 47%|████▋     | 149/320 [10:40<03:45,  1.32s/it]
 47%|████▋     | 150/320 [10:41<03:41,  1.30s/it]
 47%|████▋     | 151/320 [10:42<03:38,  1.30s/it]
 48%|████▊     | 152/320 [10:43<03:36,  1.29s/it]
 48%|████▊     | 153/320 [10:45<03:34,  1.28s/it]
 48%|████▊     | 154/320 [10:46<03:32,  1.28s/it]
 48%|████▊     | 155/320 [10:47<03:30,  1.27s/it]
 49%|████▉     | 156/320 [10:48<03:28,  1.27s/it]
 49%|████▉     | 157/320 [10:50<03:27,  1.27s/it]
 49%|████▉     | 158/320 [10:51<03:25,  1.27s/it]
 50%|████▉     | 159/320 [10:52<03:24,  1.27s/it]
 50%|█████     | 160/320 [10:53<03:23,  1.27s/it]
 50%|█████     | 161/320 [10:55<03:22,  1.27s/it]
 51%|█████     | 162/320 [10:56<03:20,  1.27s/it]
 51%|█████     | 163/320 [10:57<03:19,  1.27s/it]
 51%|█████▏    | 164/320 [10:59<03:18,  1.27s/it]
 52%|█████▏    | 165/320 [11:00<03:17,  1.27s/it]
 52%|█████▏    | 166/320 [11:01<03:16,  1.27s/it]
 52%|█████▏    | 167/320 [11:02<03:14,  1.27s/it]
 52%|█████▎    | 168/320 [11:04<03:13,  1.27s/it]
 53%|█████▎    | 169/320 [11:05<03:12,  1.28s/it]
 53%|█████▎    | 170/320 [11:06<03:11,  1.28s/it]
 53%|█████▎    | 171/320 [11:07<03:10,  1.28s/it]
 54%|█████▍    | 172/320 [11:09<03:08,  1.28s/it]
 54%|█████▍    | 173/320 [11:10<03:07,  1.28s/it]
 54%|█████▍    | 174/320 [11:11<03:07,  1.28s/it]
 55%|█████▍    | 175/320 [11:13<03:05,  1.28s/it]
 55%|█████▌    | 176/320 [11:14<03:04,  1.28s/it]
 55%|█████▌    | 177/320 [11:15<03:02,  1.28s/it]
 56%|█████▌    | 178/320 [11:16<03:01,  1.28s/it]
 56%|█████▌    | 179/320 [11:18<03:00,  1.28s/it]
 56%|█████▋    | 180/320 [11:19<02:58,  1.28s/it]
 57%|█████▋    | 181/320 [11:20<02:57,  1.28s/it]
 57%|█████▋    | 182/320 [11:22<02:56,  1.28s/it]
 57%|█████▋    | 183/320 [11:23<02:55,  1.28s/it]
 57%|█████▊    | 184/320 [11:24<02:53,  1.28s/it]
 58%|█████▊    | 185/320 [11:25<02:52,  1.28s/it]
 58%|█████▊    | 186/320 [11:27<02:51,  1.28s/it]
 58%|█████▊    | 187/320 [11:28<02:50,  1.28s/it]
 59%|█████▉    | 188/320 [11:29<02:48,  1.28s/it]
 59%|█████▉    | 189/320 [11:31<02:47,  1.28s/it]
 59%|█████▉    | 190/320 [11:32<02:46,  1.28s/it]
 60%|█████▉    | 191/320 [11:33<02:45,  1.28s/it]
 60%|██████    | 192/320 [11:34<02:24,  1.13s/it]***** Running Evaluation *****
  Num examples = 1004
  Batch size = 64
{'eval_loss': 1.0779610872268677, 'eval_runtime': 5.0981, 'eval_samples_per_second': 196.937, 'eval_steps_per_second': 3.138, 'epoch': 2.0}


  0%|          | 0/16 [00:00<?, ?it/s][A

 12%|█▎        | 2/16 [00:00<00:02,  6.19it/s][A

 19%|█▉        | 3/16 [00:00<00:02,  4.33it/s][A

 25%|██▌       | 4/16 [00:00<00:03,  3.77it/s][A

 31%|███▏      | 5/16 [00:01<00:03,  3.50it/s][A

 38%|███▊      | 6/16 [00:01<00:02,  3.34it/s][A

 44%|████▍     | 7/16 [00:01<00:02,  3.26it/s][A

 50%|█████     | 8/16 [00:02<00:02,  3.20it/s][A

 56%|█████▋    | 9/16 [00:02<00:02,  3.16it/s][A

 62%|██████▎   | 10/16 [00:02<00:01,  3.14it/s][A

 69%|██████▉   | 11/16 [00:03<00:01,  3.12it/s][A

 75%|███████▌  | 12/16 [00:03<00:01,  3.11it/s][A

 81%|████████▏ | 13/16 [00:03<00:00,  3.10it/s][A

 88%|████████▊ | 14/16 [00:04<00:00,  3.10it/s][A

 94%|█████████▍| 15/16 [00:04<00:00,  3.08it/s][A

100%|██████████| 16/16 [00:04<00:00,  3.36it/s][A
                                                 

                                               
[A
 60%|██████    | 192/320 [11:39<02:24,  1.13s/it]

100%|██████████| 16/16 [00:04<00:00,  3.36it/s][A

                                               [ASaving model checkpoint to results/checkpoints/20q/gpt2-xl/combined/checkpoint-192
Configuration saved in results/checkpoints/20q/gpt2-xl/combined/checkpoint-192/config.json
Model weights saved in results/checkpoints/20q/gpt2-xl/combined/checkpoint-192/pytorch_model.bin
Deleting older checkpoint [results/checkpoints/20q/gpt2-xl/combined/checkpoint-128] due to args.save_total_limit

 60%|██████    | 193/320 [14:57<2:10:50, 61.81s/it]
 61%|██████    | 194/320 [14:59<1:31:39, 43.64s/it]
 61%|██████    | 195/320 [15:00<1:04:25, 30.92s/it]
 61%|██████▏   | 196/320 [15:01<45:30, 22.02s/it]  
 62%|██████▏   | 197/320 [15:02<32:22, 15.79s/it]
 62%|██████▏   | 198/320 [15:04<23:14, 11.43s/it]
 62%|██████▏   | 199/320 [15:05<16:53,  8.37s/it]
 62%|██████▎   | 200/320 [15:06<12:28,  6.24s/it]
 63%|██████▎   | 201/320 [15:07<09:24,  4.74s/it]
 63%|██████▎   | 202/320 [15:09<07:16,  3.70s/it]
 63%|██████▎   | 203/320 [15:10<05:46,  2.96s/it]
 64%|██████▍   | 204/320 [15:11<04:44,  2.45s/it]
 64%|██████▍   | 205/320 [15:12<04:00,  2.09s/it]
 64%|██████▍   | 206/320 [15:14<03:29,  1.84s/it]
 65%|██████▍   | 207/320 [15:15<03:08,  1.67s/it]
 65%|██████▌   | 208/320 [15:16<02:52,  1.54s/it]
 65%|██████▌   | 209/320 [15:17<02:41,  1.46s/it]
 66%|██████▌   | 210/320 [15:19<02:33,  1.40s/it]
 66%|██████▌   | 211/320 [15:20<02:27,  1.36s/it]
 66%|██████▋   | 212/320 [15:21<02:23,  1.33s/it]
 67%|██████▋   | 213/320 [15:22<02:19,  1.31s/it]
 67%|██████▋   | 214/320 [15:24<02:17,  1.29s/it]
 67%|██████▋   | 215/320 [15:25<02:14,  1.28s/it]
 68%|██████▊   | 216/320 [15:26<02:12,  1.28s/it]
 68%|██████▊   | 217/320 [15:27<02:11,  1.27s/it]
 68%|██████▊   | 218/320 [15:29<02:09,  1.27s/it]
 68%|██████▊   | 219/320 [15:30<02:08,  1.27s/it]
 69%|██████▉   | 220/320 [15:31<02:07,  1.27s/it]
 69%|██████▉   | 221/320 [15:32<02:05,  1.27s/it]
 69%|██████▉   | 222/320 [15:34<02:04,  1.27s/it]
 70%|██████▉   | 223/320 [15:35<02:03,  1.27s/it]
 70%|███████   | 224/320 [15:36<02:01,  1.27s/it]
 70%|███████   | 225/320 [15:38<02:00,  1.27s/it]
 71%|███████   | 226/320 [15:39<01:59,  1.27s/it]
 71%|███████   | 227/320 [15:40<01:58,  1.27s/it]
 71%|███████▏  | 228/320 [15:41<01:57,  1.27s/it]
 72%|███████▏  | 229/320 [15:43<01:55,  1.27s/it]
 72%|███████▏  | 230/320 [15:44<01:54,  1.28s/it]
 72%|███████▏  | 231/320 [15:45<01:53,  1.28s/it]
 72%|███████▎  | 232/320 [15:46<01:52,  1.27s/it]
 73%|███████▎  | 233/320 [15:48<01:50,  1.27s/it]
 73%|███████▎  | 234/320 [15:49<01:49,  1.27s/it]
 73%|███████▎  | 235/320 [15:50<01:48,  1.27s/it]
 74%|███████▍  | 236/320 [15:52<01:47,  1.27s/it]
 74%|███████▍  | 237/320 [15:53<01:45,  1.28s/it]
 74%|███████▍  | 238/320 [15:54<01:44,  1.28s/it]
 75%|███████▍  | 239/320 [15:55<01:43,  1.28s/it]
 75%|███████▌  | 240/320 [15:57<01:42,  1.28s/it]
 75%|███████▌  | 241/320 [15:58<01:40,  1.28s/it]
 76%|███████▌  | 242/320 [15:59<01:39,  1.28s/it]
 76%|███████▌  | 243/320 [16:01<01:38,  1.28s/it]
 76%|███████▋  | 244/320 [16:02<01:37,  1.28s/it]
 77%|███████▋  | 245/320 [16:03<01:35,  1.28s/it]
 77%|███████▋  | 246/320 [16:04<01:34,  1.28s/it]
 77%|███████▋  | 247/320 [16:06<01:33,  1.28s/it]
 78%|███████▊  | 248/320 [16:07<01:32,  1.28s/it]
 78%|███████▊  | 249/320 [16:08<01:30,  1.28s/it]
 78%|███████▊  | 250/320 [16:09<01:29,  1.28s/it]
 78%|███████▊  | 251/320 [16:11<01:28,  1.28s/it]
 79%|███████▉  | 252/320 [16:12<01:26,  1.28s/it]
 79%|███████▉  | 253/320 [16:13<01:25,  1.28s/it]
 79%|███████▉  | 254/320 [16:15<01:24,  1.28s/it]
 80%|███████▉  | 255/320 [16:16<01:23,  1.28s/it]
 80%|████████  | 256/320 [16:17<01:12,  1.13s/it]***** Running Evaluation *****
  Num examples = 1004
  Batch size = 64
{'eval_loss': 1.1770559549331665, 'eval_runtime': 5.1002, 'eval_samples_per_second': 196.857, 'eval_steps_per_second': 3.137, 'epoch': 3.0}


  0%|          | 0/16 [00:00<?, ?it/s][A

 12%|█▎        | 2/16 [00:00<00:02,  6.21it/s][A

 19%|█▉        | 3/16 [00:00<00:03,  4.33it/s][A

 25%|██▌       | 4/16 [00:00<00:03,  3.76it/s][A

 31%|███▏      | 5/16 [00:01<00:03,  3.49it/s][A

 38%|███▊      | 6/16 [00:01<00:02,  3.34it/s][A

 44%|████▍     | 7/16 [00:01<00:02,  3.25it/s][A

 50%|█████     | 8/16 [00:02<00:02,  3.20it/s][A

 56%|█████▋    | 9/16 [00:02<00:02,  3.16it/s][A

 62%|██████▎   | 10/16 [00:02<00:01,  3.13it/s][A

 69%|██████▉   | 11/16 [00:03<00:01,  3.11it/s][A

 75%|███████▌  | 12/16 [00:03<00:01,  3.10it/s][A

 81%|████████▏ | 13/16 [00:03<00:00,  3.09it/s][A

 88%|████████▊ | 14/16 [00:04<00:00,  3.09it/s][A

 94%|█████████▍| 15/16 [00:04<00:00,  3.09it/s][A

100%|██████████| 16/16 [00:04<00:00,  3.37it/s][A
                                                 

                                               
[A
 80%|████████  | 256/320 [16:22<01:12,  1.13s/it]

100%|██████████| 16/16 [00:04<00:00,  3.37it/s][A

                                               [ASaving model checkpoint to results/checkpoints/20q/gpt2-xl/combined/checkpoint-256
Configuration saved in results/checkpoints/20q/gpt2-xl/combined/checkpoint-256/config.json
Model weights saved in results/checkpoints/20q/gpt2-xl/combined/checkpoint-256/pytorch_model.bin
Deleting older checkpoint [results/checkpoints/20q/gpt2-xl/combined/checkpoint-192] due to args.save_total_limit

 80%|████████  | 257/320 [20:27<1:19:48, 76.01s/it]
 81%|████████  | 258/320 [20:29<55:21, 53.58s/it]  
 81%|████████  | 259/320 [20:30<38:30, 37.88s/it]
 81%|████████▏ | 260/320 [20:31<26:53, 26.89s/it]
 82%|████████▏ | 261/320 [20:32<18:52, 19.20s/it]
 82%|████████▏ | 262/320 [20:34<13:21, 13.81s/it]
 82%|████████▏ | 263/320 [20:35<09:32, 10.05s/it]
 82%|████████▎ | 264/320 [20:36<06:54,  7.41s/it]
 83%|████████▎ | 265/320 [20:37<05:05,  5.56s/it]
 83%|████████▎ | 266/320 [20:39<03:50,  4.27s/it]
 83%|████████▎ | 267/320 [20:40<02:58,  3.37s/it]
 84%|████████▍ | 268/320 [20:41<02:22,  2.73s/it]
 84%|████████▍ | 269/320 [20:42<01:56,  2.29s/it]
 84%|████████▍ | 270/320 [20:44<01:38,  1.98s/it]
 85%|████████▍ | 271/320 [20:45<01:26,  1.76s/it]
 85%|████████▌ | 272/320 [20:46<01:17,  1.61s/it]
 85%|████████▌ | 273/320 [20:47<01:10,  1.51s/it]
 86%|████████▌ | 274/320 [20:49<01:05,  1.43s/it]
 86%|████████▌ | 275/320 [20:50<01:02,  1.38s/it]
 86%|████████▋ | 276/320 [20:51<00:59,  1.34s/it]
 87%|████████▋ | 277/320 [20:52<00:56,  1.32s/it]
 87%|████████▋ | 278/320 [20:54<00:54,  1.30s/it]
 87%|████████▋ | 279/320 [20:55<00:52,  1.29s/it]
 88%|████████▊ | 280/320 [20:56<00:51,  1.28s/it]
 88%|████████▊ | 281/320 [20:58<00:49,  1.28s/it]
 88%|████████▊ | 282/320 [20:59<00:48,  1.27s/it]
 88%|████████▊ | 283/320 [21:00<00:47,  1.27s/it]
 89%|████████▉ | 284/320 [21:01<00:45,  1.27s/it]
 89%|████████▉ | 285/320 [21:03<00:44,  1.27s/it]
 89%|████████▉ | 286/320 [21:04<00:43,  1.27s/it]
 90%|████████▉ | 287/320 [21:05<00:41,  1.27s/it]
 90%|█████████ | 288/320 [21:06<00:40,  1.27s/it]
 90%|█████████ | 289/320 [21:08<00:39,  1.27s/it]
 91%|█████████ | 290/320 [21:09<00:38,  1.27s/it]
 91%|█████████ | 291/320 [21:10<00:36,  1.27s/it]
 91%|█████████▏| 292/320 [21:11<00:35,  1.27s/it]
 92%|█████████▏| 293/320 [21:13<00:34,  1.27s/it]
 92%|█████████▏| 294/320 [21:14<00:33,  1.27s/it]
 92%|█████████▏| 295/320 [21:15<00:31,  1.27s/it]
 92%|█████████▎| 296/320 [21:17<00:30,  1.27s/it]
 93%|█████████▎| 297/320 [21:18<00:29,  1.27s/it]
 93%|█████████▎| 298/320 [21:19<00:28,  1.27s/it]
 93%|█████████▎| 299/320 [21:20<00:26,  1.27s/it]
 94%|█████████▍| 300/320 [21:22<00:25,  1.27s/it]
 94%|█████████▍| 301/320 [21:23<00:24,  1.27s/it]
 94%|█████████▍| 302/320 [21:24<00:22,  1.27s/it]
 95%|█████████▍| 303/320 [21:25<00:21,  1.28s/it]
 95%|█████████▌| 304/320 [21:27<00:20,  1.28s/it]
 95%|█████████▌| 305/320 [21:28<00:19,  1.28s/it]
 96%|█████████▌| 306/320 [21:29<00:17,  1.28s/it]
 96%|█████████▌| 307/320 [21:31<00:16,  1.28s/it]
 96%|█████████▋| 308/320 [21:32<00:15,  1.28s/it]
 97%|█████████▋| 309/320 [21:33<00:14,  1.28s/it]
 97%|█████████▋| 310/320 [21:34<00:12,  1.28s/it]
 97%|█████████▋| 311/320 [21:36<00:11,  1.28s/it]
 98%|█████████▊| 312/320 [21:37<00:10,  1.28s/it]
 98%|█████████▊| 313/320 [21:38<00:08,  1.28s/it]
 98%|█████████▊| 314/320 [21:40<00:07,  1.28s/it]
 98%|█████████▊| 315/320 [21:41<00:06,  1.28s/it]
 99%|█████████▉| 316/320 [21:42<00:05,  1.28s/it]
 99%|█████████▉| 317/320 [21:43<00:03,  1.28s/it]
 99%|█████████▉| 318/320 [21:45<00:02,  1.28s/it]
100%|█████████▉| 319/320 [21:46<00:01,  1.28s/it]
100%|██████████| 320/320 [21:47<00:00,  1.13s/it]***** Running Evaluation *****
  Num examples = 1004
  Batch size = 64
{'eval_loss': 1.2427932024002075, 'eval_runtime': 5.1029, 'eval_samples_per_second': 196.75, 'eval_steps_per_second': 3.135, 'epoch': 4.0}


  0%|          | 0/16 [00:00<?, ?it/s][A

 12%|█▎        | 2/16 [00:00<00:02,  6.20it/s][A

 19%|█▉        | 3/16 [00:00<00:03,  4.33it/s][A

 25%|██▌       | 4/16 [00:00<00:03,  3.77it/s][A

 31%|███▏      | 5/16 [00:01<00:03,  3.51it/s][A

 38%|███▊      | 6/16 [00:01<00:02,  3.35it/s][A

 44%|████▍     | 7/16 [00:01<00:02,  3.26it/s][A

 50%|█████     | 8/16 [00:02<00:02,  3.21it/s][A

 56%|█████▋    | 9/16 [00:02<00:02,  3.17it/s][A

 62%|██████▎   | 10/16 [00:02<00:01,  3.14it/s][A

 69%|██████▉   | 11/16 [00:03<00:01,  3.11it/s][A

 75%|███████▌  | 12/16 [00:03<00:01,  3.10it/s][A

 81%|████████▏ | 13/16 [00:03<00:00,  3.09it/s][A

 88%|████████▊ | 14/16 [00:04<00:00,  3.09it/s][A

 94%|█████████▍| 15/16 [00:04<00:00,  3.09it/s][A

100%|██████████| 16/16 [00:04<00:00,  3.37it/s][A
                                                 

                                               
[A
100%|██████████| 320/320 [21:52<00:00,  1.13s/it]

100%|██████████| 16/16 [00:04<00:00,  3.37it/s][A

                                               [ASaving model checkpoint to results/checkpoints/20q/gpt2-xl/combined/checkpoint-320
Configuration saved in results/checkpoints/20q/gpt2-xl/combined/checkpoint-320/config.json
Model weights saved in results/checkpoints/20q/gpt2-xl/combined/checkpoint-320/pytorch_model.bin
Deleting older checkpoint [results/checkpoints/20q/gpt2-xl/combined/checkpoint-256] due to args.save_total_limit


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from results/checkpoints/20q/gpt2-xl/combined/checkpoint-64 (score: 1.0134471654891968).

                                                 

100%|██████████| 320/320 [27:19<00:00,  1.13s/it]
100%|██████████| 320/320 [27:19<00:00,  5.12s/it]
***** Running Evaluation *****
  Num examples = 1004
  Batch size = 64
{'eval_loss': 1.316015362739563, 'eval_runtime': 5.0966, 'eval_samples_per_second': 196.993, 'eval_steps_per_second': 3.139, 'epoch': 5.0}
{'train_runtime': 1639.5018, 'train_samples_per_second': 12.382, 'train_steps_per_second': 0.195, 'train_loss': 0.6222230911254882, 'epoch': 5.0}

  0%|          | 0/16 [00:00<?, ?it/s]
 12%|█▎        | 2/16 [00:00<00:02,  6.44it/s]
 19%|█▉        | 3/16 [00:00<00:02,  4.52it/s]
 25%|██▌       | 4/16 [00:00<00:03,  3.91it/s]
 31%|███▏      | 5/16 [00:01<00:03,  3.63it/s]
 38%|███▊      | 6/16 [00:01<00:02,  3.48it/s]
 44%|████▍     | 7/16 [00:01<00:02,  3.38it/s]
 50%|█████     | 8/16 [00:02<00:02,  3.32it/s]
 56%|█████▋    | 9/16 [00:02<00:02,  3.28it/s]
 62%|██████▎   | 10/16 [00:02<00:01,  3.24it/s]
 69%|██████▉   | 11/16 [00:03<00:01,  3.22it/s]
 75%|███████▌  | 12/16 [00:03<00:01,  3.21it/s]
 81%|████████▏ | 13/16 [00:03<00:00,  3.20it/s]
 88%|████████▊ | 14/16 [00:04<00:00,  3.20it/s]
 94%|█████████▍| 15/16 [00:04<00:00,  3.20it/s]
100%|██████████| 16/16 [00:04<00:00,  3.49it/s]
100%|██████████| 16/16 [00:04<00:00,  3.46it/s]
Saving model checkpoint to results/best-checkpoints/20q/gpt2-xl/combined
Configuration saved in results/best-checkpoints/20q/gpt2-xl/combined/config.json
Model weights saved in results/best-checkpoints/20q/gpt2-xl/combined/pytorch_model.bin
loading configuration file results/best-checkpoints/20q/gpt2-xl/combined/config.json
Model config GPT2Config {
  "_name_or_path": "results/best-checkpoints/20q/gpt2-xl/combined",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1600,
  "n_head": 25,
  "n_inner": null,
  "n_layer": 48,
  "n_positions": 1024,
  "output_past": true,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.23.1",
  "use_cache": true,
  "vocab_size": 50257
}

loading weights file results/best-checkpoints/20q/gpt2-xl/combined/pytorch_model.bin
All model checkpoint weights were used when initializing GPT2LMHeadModel.

All the weights of GPT2LMHeadModel were initialized from the model checkpoint at results/best-checkpoints/20q/gpt2-xl/combined.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /work/anshitagupta_umass_edu/.cache/huggingface/hub/models--gpt2-xl/snapshots/33cdb5c0db5423c1879b1b9f16c352988e8754a8/config.json
Model config GPT2Config {
  "_name_or_path": "gpt2-xl",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1600,
  "n_head": 25,
  "n_inner": null,
  "n_layer": 48,
  "n_positions": 1024,
  "output_past": true,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.23.1",
  "use_cache": true,
  "vocab_size": 50257
}

loading file vocab.json from cache at /work/anshitagupta_umass_edu/.cache/huggingface/hub/models--gpt2-xl/snapshots/33cdb5c0db5423c1879b1b9f16c352988e8754a8/vocab.json
loading file merges.txt from cache at /work/anshitagupta_umass_edu/.cache/huggingface/hub/models--gpt2-xl/snapshots/33cdb5c0db5423c1879b1b9f16c352988e8754a8/merges.txt
loading file tokenizer.json from cache at /work/anshitagupta_umass_edu/.cache/huggingface/hub/models--gpt2-xl/snapshots/33cdb5c0db5423c1879b1b9f16c352988e8754a8/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /work/anshitagupta_umass_edu/.cache/huggingface/hub/models--gpt2-xl/snapshots/33cdb5c0db5423c1879b1b9f16c352988e8754a8/config.json
Model config GPT2Config {
  "_name_or_path": "gpt2-xl",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1600,
  "n_head": 25,
  "n_inner": null,
  "n_layer": 48,
  "n_positions": 1024,
  "output_past": true,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.23.1",
  "use_cache": true,
  "vocab_size": 50257
}

{'eval_loss': 1.0134471654891968, 'eval_runtime': 4.9447, 'eval_samples_per_second': 203.048, 'eval_steps_per_second': 3.236, 'epoch': 5.0}
Loaded dataset with 4060 elements
Loaded dataset with 1004 elements
Loaded dataset with 5055 elements
train Accuracy =  0.8369458128078818
train F1 score =  0.6011407732134714
train Confusion Matrix = 
 [[1475   46  493]
 [   0 1923  123]
 [   0    0    0]]
Classification Report: 
               precision    recall  f1-score   support

        True       1.00      0.73      0.85      2014
       False       0.98      0.94      0.96      2046
        None       0.00      1.00      0.00         0

    accuracy                           0.84      4060
   macro avg       0.66      0.89      0.60      4060
weighted avg       0.99      0.84      0.90      4060

valid Accuracy =  0.8645418326693227
valid F1 score =  0.6152085774859956
valid Confusion Matrix = 
 [[393   5 104]
 [  0 475  27]
 [  0   0   0]]
Classification Report: 
               precision    recall  f1-score   support

        True       1.00      0.78      0.88       502
       False       0.99      0.95      0.97       502
        None       0.00      1.00      0.00         0

    accuracy                           0.86      1004
   macro avg       0.66      0.91      0.62      1004
weighted avg       0.99      0.86      0.92      1004

test Accuracy =  0.8340257171117705
test F1 score =  0.5998644434832512
test Confusion Matrix = 
 [[1847   51  609]
 [  13 2369  166]
 [   0    0    0]]
Classification Report: 
               precision    recall  f1-score   support

        True       0.99      0.74      0.85      2507
       False       0.98      0.93      0.95      2548
        None       0.00      1.00      0.00         0

    accuracy                           0.83      5055
   macro avg       0.66      0.89      0.60      5055
weighted avg       0.99      0.83      0.90      5055

