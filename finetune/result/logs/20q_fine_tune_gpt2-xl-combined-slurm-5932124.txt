/work/anshitagupta_umass_edu/miniconda3/envs/memit/lib/python3.9/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning
  warnings.warn(
***** Running training *****
  Num examples = 4060
  Num Epochs = 5
  Instantaneous batch size per device = 64
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 1
  Total optimization steps = 320
Loaded dataset with 4060 elements
Loaded dataset with 1004 elements

  0%|          | 0/320 [00:00<?, ?it/s]
  0%|          | 1/320 [00:02<11:46,  2.22s/it]
  1%|          | 2/320 [00:03<08:44,  1.65s/it]
  1%|          | 3/320 [00:04<07:44,  1.47s/it]
  1%|â–         | 4/320 [00:05<07:16,  1.38s/it]
  2%|â–         | 5/320 [00:07<06:59,  1.33s/it]
  2%|â–         | 6/320 [00:08<06:49,  1.30s/it]
  2%|â–         | 7/320 [00:09<06:42,  1.29s/it]
  2%|â–Ž         | 8/320 [00:10<06:38,  1.28s/it]
  3%|â–Ž         | 9/320 [00:12<06:34,  1.27s/it]
  3%|â–Ž         | 10/320 [00:13<06:32,  1.26s/it]
  3%|â–Ž         | 11/320 [00:14<06:29,  1.26s/it]
  4%|â–         | 12/320 [00:15<06:28,  1.26s/it]
  4%|â–         | 13/320 [00:17<06:26,  1.26s/it]
  4%|â–         | 14/320 [00:18<06:25,  1.26s/it]
  5%|â–         | 15/320 [00:19<06:24,  1.26s/it]
  5%|â–Œ         | 16/320 [00:21<06:23,  1.26s/it]
  5%|â–Œ         | 17/320 [00:22<06:21,  1.26s/it]
  6%|â–Œ         | 18/320 [00:23<06:20,  1.26s/it]
  6%|â–Œ         | 19/320 [00:24<06:19,  1.26s/it]
  6%|â–‹         | 20/320 [00:26<06:18,  1.26s/it]
  7%|â–‹         | 21/320 [00:27<06:17,  1.26s/it]
  7%|â–‹         | 22/320 [00:28<06:16,  1.26s/it]
  7%|â–‹         | 23/320 [00:29<06:14,  1.26s/it]
  8%|â–Š         | 24/320 [00:31<06:13,  1.26s/it]
  8%|â–Š         | 25/320 [00:32<06:12,  1.26s/it]
  8%|â–Š         | 26/320 [00:33<06:11,  1.26s/it]
  8%|â–Š         | 27/320 [00:34<06:10,  1.27s/it]
  9%|â–‰         | 28/320 [00:36<06:09,  1.27s/it]
  9%|â–‰         | 29/320 [00:37<06:08,  1.27s/it]
  9%|â–‰         | 30/320 [00:38<06:07,  1.27s/it]
 10%|â–‰         | 31/320 [00:39<06:06,  1.27s/it]
 10%|â–ˆ         | 32/320 [00:41<06:05,  1.27s/it]
 10%|â–ˆ         | 33/320 [00:42<06:04,  1.27s/it]
 11%|â–ˆ         | 34/320 [00:43<06:03,  1.27s/it]
 11%|â–ˆ         | 35/320 [00:45<06:01,  1.27s/it]
 11%|â–ˆâ–        | 36/320 [00:46<06:00,  1.27s/it]
 12%|â–ˆâ–        | 37/320 [00:47<05:59,  1.27s/it]
 12%|â–ˆâ–        | 38/320 [00:48<05:58,  1.27s/it]
 12%|â–ˆâ–        | 39/320 [00:50<05:57,  1.27s/it]
 12%|â–ˆâ–Ž        | 40/320 [00:51<05:56,  1.27s/it]
 13%|â–ˆâ–Ž        | 41/320 [00:52<05:54,  1.27s/it]
 13%|â–ˆâ–Ž        | 42/320 [00:53<05:53,  1.27s/it]
 13%|â–ˆâ–Ž        | 43/320 [00:55<05:52,  1.27s/it]
 14%|â–ˆâ–        | 44/320 [00:56<05:51,  1.27s/it]
 14%|â–ˆâ–        | 45/320 [00:57<05:50,  1.27s/it]
 14%|â–ˆâ–        | 46/320 [00:59<05:49,  1.27s/it]
 15%|â–ˆâ–        | 47/320 [01:00<05:47,  1.27s/it]
 15%|â–ˆâ–Œ        | 48/320 [01:01<05:46,  1.28s/it]
 15%|â–ˆâ–Œ        | 49/320 [01:02<05:45,  1.28s/it]
 16%|â–ˆâ–Œ        | 50/320 [01:04<05:44,  1.28s/it]
 16%|â–ˆâ–Œ        | 51/320 [01:05<05:43,  1.28s/it]
 16%|â–ˆâ–‹        | 52/320 [01:06<05:41,  1.28s/it]
 17%|â–ˆâ–‹        | 53/320 [01:08<05:40,  1.28s/it]
 17%|â–ˆâ–‹        | 54/320 [01:09<05:39,  1.28s/it]
 17%|â–ˆâ–‹        | 55/320 [01:10<05:38,  1.28s/it]
 18%|â–ˆâ–Š        | 56/320 [01:11<05:37,  1.28s/it]
 18%|â–ˆâ–Š        | 57/320 [01:13<05:35,  1.28s/it]
 18%|â–ˆâ–Š        | 58/320 [01:14<05:34,  1.28s/it]
 18%|â–ˆâ–Š        | 59/320 [01:15<05:33,  1.28s/it]
 19%|â–ˆâ–‰        | 60/320 [01:16<05:32,  1.28s/it]
 19%|â–ˆâ–‰        | 61/320 [01:18<05:31,  1.28s/it]
 19%|â–ˆâ–‰        | 62/320 [01:19<05:29,  1.28s/it]
 20%|â–ˆâ–‰        | 63/320 [01:20<05:28,  1.28s/it]
 20%|â–ˆâ–ˆ        | 64/320 [01:21<04:51,  1.14s/it]***** Running Evaluation *****
  Num examples = 1004
  Batch size = 64


  0%|          | 0/16 [00:00<?, ?it/s][A

 12%|â–ˆâ–Ž        | 2/16 [00:00<00:02,  6.21it/s][A

 19%|â–ˆâ–‰        | 3/16 [00:00<00:02,  4.35it/s][A

 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:00<00:03,  3.78it/s][A

 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:03,  3.51it/s][A

 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  3.35it/s][A

 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  3.26it/s][A

 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.20it/s][A

 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:02,  3.17it/s][A

 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.14it/s][A

 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:03<00:01,  3.12it/s][A

 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.12it/s][A

 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.11it/s][A

 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:04<00:00,  3.10it/s][A

 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:04<00:00,  3.10it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.38it/s][A
                                                

                                               
[A
 20%|â–ˆâ–ˆ        | 64/320 [01:26<04:51,  1.14s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.38it/s][A

                                               [ASaving model checkpoint to results/checkpoints/20q/gpt2-xl/combined/checkpoint-64
Configuration saved in results/checkpoints/20q/gpt2-xl/combined/checkpoint-64/config.json
Model weights saved in results/checkpoints/20q/gpt2-xl/combined/checkpoint-64/pytorch_model.bin

 20%|â–ˆâ–ˆ        | 65/320 [05:12<4:58:21, 70.20s/it]
 21%|â–ˆâ–ˆ        | 66/320 [05:14<3:29:36, 49.51s/it]
 21%|â–ˆâ–ˆ        | 67/320 [05:15<2:27:43, 35.03s/it]
 21%|â–ˆâ–ˆâ–       | 68/320 [05:16<1:44:34, 24.90s/it]
 22%|â–ˆâ–ˆâ–       | 69/320 [05:17<1:14:28, 17.80s/it]
 22%|â–ˆâ–ˆâ–       | 70/320 [05:19<53:29, 12.84s/it]  
 22%|â–ˆâ–ˆâ–       | 71/320 [05:20<38:50,  9.36s/it]
 22%|â–ˆâ–ˆâ–Ž       | 72/320 [05:21<28:38,  6.93s/it]
 23%|â–ˆâ–ˆâ–Ž       | 73/320 [05:22<21:30,  5.23s/it]
 23%|â–ˆâ–ˆâ–Ž       | 74/320 [05:24<16:33,  4.04s/it]
 23%|â–ˆâ–ˆâ–Ž       | 75/320 [05:25<13:04,  3.20s/it]
 24%|â–ˆâ–ˆâ–       | 76/320 [05:26<10:38,  2.62s/it]
 24%|â–ˆâ–ˆâ–       | 77/320 [05:27<08:56,  2.21s/it]
 24%|â–ˆâ–ˆâ–       | 78/320 [05:29<07:45,  1.92s/it]
 25%|â–ˆâ–ˆâ–       | 79/320 [05:30<06:54,  1.72s/it]
 25%|â–ˆâ–ˆâ–Œ       | 80/320 [05:31<06:19,  1.58s/it]
 25%|â–ˆâ–ˆâ–Œ       | 81/320 [05:32<05:54,  1.49s/it]
 26%|â–ˆâ–ˆâ–Œ       | 82/320 [05:34<05:37,  1.42s/it]
 26%|â–ˆâ–ˆâ–Œ       | 83/320 [05:35<05:25,  1.37s/it]
 26%|â–ˆâ–ˆâ–‹       | 84/320 [05:36<05:16,  1.34s/it]
 27%|â–ˆâ–ˆâ–‹       | 85/320 [05:38<05:09,  1.32s/it]
 27%|â–ˆâ–ˆâ–‹       | 86/320 [05:39<05:04,  1.30s/it]
 27%|â–ˆâ–ˆâ–‹       | 87/320 [05:40<05:00,  1.29s/it]
 28%|â–ˆâ–ˆâ–Š       | 88/320 [05:41<04:57,  1.28s/it]
 28%|â–ˆâ–ˆâ–Š       | 89/320 [05:43<04:54,  1.28s/it]
 28%|â–ˆâ–ˆâ–Š       | 90/320 [05:44<04:52,  1.27s/it]
 28%|â–ˆâ–ˆâ–Š       | 91/320 [05:45<04:50,  1.27s/it]
 29%|â–ˆâ–ˆâ–‰       | 92/320 [05:46<04:49,  1.27s/it]
 29%|â–ˆâ–ˆâ–‰       | 93/320 [05:48<04:47,  1.27s/it]
 29%|â–ˆâ–ˆâ–‰       | 94/320 [05:49<04:46,  1.27s/it]
 30%|â–ˆâ–ˆâ–‰       | 95/320 [05:50<04:46,  1.27s/it]
 30%|â–ˆâ–ˆâ–ˆ       | 96/320 [05:51<04:44,  1.27s/it]
 30%|â–ˆâ–ˆâ–ˆ       | 97/320 [05:53<04:43,  1.27s/it]
 31%|â–ˆâ–ˆâ–ˆ       | 98/320 [05:54<04:42,  1.27s/it]
 31%|â–ˆâ–ˆâ–ˆ       | 99/320 [05:55<04:40,  1.27s/it]
 31%|â–ˆâ–ˆâ–ˆâ–      | 100/320 [05:57<04:39,  1.27s/it]
 32%|â–ˆâ–ˆâ–ˆâ–      | 101/320 [05:58<04:38,  1.27s/it]
 32%|â–ˆâ–ˆâ–ˆâ–      | 102/320 [05:59<04:37,  1.27s/it]
 32%|â–ˆâ–ˆâ–ˆâ–      | 103/320 [06:00<04:36,  1.27s/it]
 32%|â–ˆâ–ˆâ–ˆâ–Ž      | 104/320 [06:02<04:35,  1.27s/it]
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 105/320 [06:03<04:33,  1.27s/it]
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 106/320 [06:04<04:32,  1.27s/it]
 33%|â–ˆâ–ˆâ–ˆâ–Ž      | 107/320 [06:05<04:31,  1.27s/it]
 34%|â–ˆâ–ˆâ–ˆâ–      | 108/320 [06:07<04:30,  1.27s/it]
 34%|â–ˆâ–ˆâ–ˆâ–      | 109/320 [06:08<04:28,  1.27s/it]
 34%|â–ˆâ–ˆâ–ˆâ–      | 110/320 [06:09<04:27,  1.27s/it]
 35%|â–ˆâ–ˆâ–ˆâ–      | 111/320 [06:11<04:26,  1.27s/it]
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 112/320 [06:12<04:25,  1.27s/it]
 35%|â–ˆâ–ˆâ–ˆâ–Œ      | 113/320 [06:13<04:23,  1.28s/it]
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 114/320 [06:14<04:22,  1.28s/it]
 36%|â–ˆâ–ˆâ–ˆâ–Œ      | 115/320 [06:16<04:22,  1.28s/it]
 36%|â–ˆâ–ˆâ–ˆâ–‹      | 116/320 [06:17<04:20,  1.28s/it]
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 117/320 [06:18<04:19,  1.28s/it]
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 118/320 [06:20<04:18,  1.28s/it]
 37%|â–ˆâ–ˆâ–ˆâ–‹      | 119/320 [06:21<04:16,  1.28s/it]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 120/320 [06:22<04:15,  1.28s/it]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 121/320 [06:23<04:14,  1.28s/it]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 122/320 [06:25<04:13,  1.28s/it]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 123/320 [06:26<04:11,  1.28s/it]
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 124/320 [06:27<04:10,  1.28s/it]
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 125/320 [06:28<04:09,  1.28s/it]
 39%|â–ˆâ–ˆâ–ˆâ–‰      | 126/320 [06:30<04:08,  1.28s/it]
 40%|â–ˆâ–ˆâ–ˆâ–‰      | 127/320 [06:31<04:06,  1.28s/it]
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 128/320 [06:32<03:36,  1.13s/it]***** Running Evaluation *****
  Num examples = 1004
  Batch size = 64
{'eval_loss': 1.0134471654891968, 'eval_runtime': 5.0875, 'eval_samples_per_second': 197.345, 'eval_steps_per_second': 3.145, 'epoch': 1.0}


  0%|          | 0/16 [00:00<?, ?it/s][A

 12%|â–ˆâ–Ž        | 2/16 [00:00<00:02,  6.20it/s][A

 19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  4.33it/s][A

 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:00<00:03,  3.77it/s][A

 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:03,  3.50it/s][A

 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  3.34it/s][A

 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  3.26it/s][A

 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.20it/s][A

 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:02,  3.16it/s][A

 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.14it/s][A

 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:03<00:01,  3.12it/s][A

 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.10it/s][A

 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.10it/s][A

 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:04<00:00,  3.09it/s][A

 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:04<00:00,  3.09it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.37it/s][A
                                                 

                                               
[A
 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 128/320 [06:37<03:36,  1.13s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.37it/s][A

                                               [ASaving model checkpoint to results/checkpoints/20q/gpt2-xl/combined/checkpoint-128
Configuration saved in results/checkpoints/20q/gpt2-xl/combined/checkpoint-128/config.json
Model weights saved in results/checkpoints/20q/gpt2-xl/combined/checkpoint-128/pytorch_model.bin

 40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 129/320 [10:14<3:35:03, 67.56s/it]
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 130/320 [10:16<2:30:55, 47.66s/it]
 41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 131/320 [10:17<1:46:16, 33.74s/it]
 41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 132/320 [10:18<1:15:10, 23.99s/it]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 133/320 [10:19<53:30, 17.17s/it]  
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 134/320 [10:21<38:25, 12.40s/it]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 135/320 [10:22<27:54,  9.05s/it]
 42%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 136/320 [10:23<20:35,  6.71s/it]
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 137/320 [10:24<15:29,  5.08s/it]
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 138/320 [10:26<11:55,  3.93s/it]
 43%|â–ˆâ–ˆâ–ˆâ–ˆâ–Ž     | 139/320 [10:27<09:26,  3.13s/it]
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 140/320 [10:28<07:42,  2.57s/it]
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 141/320 [10:29<06:29,  2.17s/it]
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 142/320 [10:31<05:38,  1.90s/it]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 143/320 [10:32<05:02,  1.71s/it]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 144/320 [10:33<04:36,  1.57s/it]
 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 145/320 [10:34<04:19,  1.48s/it]
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 146/320 [10:36<04:06,  1.41s/it]
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 147/320 [10:37<03:56,  1.37s/it]
 46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 148/320 [10:38<03:50,  1.34s/it]
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 149/320 [10:40<03:45,  1.32s/it]
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 150/320 [10:41<03:41,  1.30s/it]
 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 151/320 [10:42<03:38,  1.30s/it]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 152/320 [10:43<03:36,  1.29s/it]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 153/320 [10:45<03:34,  1.28s/it]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 154/320 [10:46<03:32,  1.28s/it]
 48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 155/320 [10:47<03:30,  1.27s/it]
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 156/320 [10:48<03:28,  1.27s/it]
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 157/320 [10:50<03:27,  1.27s/it]
 49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 158/320 [10:51<03:25,  1.27s/it]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 159/320 [10:52<03:24,  1.27s/it]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 160/320 [10:53<03:23,  1.27s/it]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 161/320 [10:55<03:22,  1.27s/it]
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 162/320 [10:56<03:20,  1.27s/it]
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 163/320 [10:57<03:19,  1.27s/it]
 51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 164/320 [10:59<03:18,  1.27s/it]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 165/320 [11:00<03:17,  1.27s/it]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 166/320 [11:01<03:16,  1.27s/it]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 167/320 [11:02<03:14,  1.27s/it]
 52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 168/320 [11:04<03:13,  1.27s/it]
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 169/320 [11:05<03:12,  1.28s/it]
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 170/320 [11:06<03:11,  1.28s/it]
 53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž    | 171/320 [11:07<03:10,  1.28s/it]
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 172/320 [11:09<03:08,  1.28s/it]
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 173/320 [11:10<03:07,  1.28s/it]
 54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 174/320 [11:11<03:07,  1.28s/it]
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 175/320 [11:13<03:05,  1.28s/it]
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 176/320 [11:14<03:04,  1.28s/it]
 55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 177/320 [11:15<03:02,  1.28s/it]
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 178/320 [11:16<03:01,  1.28s/it]
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 179/320 [11:18<03:00,  1.28s/it]
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 180/320 [11:19<02:58,  1.28s/it]
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 181/320 [11:20<02:57,  1.28s/it]
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 182/320 [11:22<02:56,  1.28s/it]
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 183/320 [11:23<02:55,  1.28s/it]
 57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 184/320 [11:24<02:53,  1.28s/it]
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 185/320 [11:25<02:52,  1.28s/it]
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 186/320 [11:27<02:51,  1.28s/it]
 58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 187/320 [11:28<02:50,  1.28s/it]
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 188/320 [11:29<02:48,  1.28s/it]
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 189/320 [11:31<02:47,  1.28s/it]
 59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 190/320 [11:32<02:46,  1.28s/it]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 191/320 [11:33<02:45,  1.28s/it]
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 192/320 [11:34<02:24,  1.13s/it]***** Running Evaluation *****
  Num examples = 1004
  Batch size = 64
{'eval_loss': 1.0779610872268677, 'eval_runtime': 5.0981, 'eval_samples_per_second': 196.937, 'eval_steps_per_second': 3.138, 'epoch': 2.0}


  0%|          | 0/16 [00:00<?, ?it/s][A

 12%|â–ˆâ–Ž        | 2/16 [00:00<00:02,  6.19it/s][A

 19%|â–ˆâ–‰        | 3/16 [00:00<00:02,  4.33it/s][A

 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:00<00:03,  3.77it/s][A

 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:03,  3.50it/s][A

 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  3.34it/s][A

 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  3.26it/s][A

 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.20it/s][A

 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:02,  3.16it/s][A

 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.14it/s][A

 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:03<00:01,  3.12it/s][A

 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.11it/s][A

 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.10it/s][A

 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:04<00:00,  3.10it/s][A

 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:04<00:00,  3.08it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.36it/s][A
                                                 

                                               
[A
 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 192/320 [11:39<02:24,  1.13s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.36it/s][A

                                               [ASaving model checkpoint to results/checkpoints/20q/gpt2-xl/combined/checkpoint-192
Configuration saved in results/checkpoints/20q/gpt2-xl/combined/checkpoint-192/config.json
Model weights saved in results/checkpoints/20q/gpt2-xl/combined/checkpoint-192/pytorch_model.bin
Deleting older checkpoint [results/checkpoints/20q/gpt2-xl/combined/checkpoint-128] due to args.save_total_limit

 60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 193/320 [14:57<2:10:50, 61.81s/it]
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 194/320 [14:59<1:31:39, 43.64s/it]
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 195/320 [15:00<1:04:25, 30.92s/it]
 61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 196/320 [15:01<45:30, 22.02s/it]  
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 197/320 [15:02<32:22, 15.79s/it]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 198/320 [15:04<23:14, 11.43s/it]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 199/320 [15:05<16:53,  8.37s/it]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 200/320 [15:06<12:28,  6.24s/it]
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 201/320 [15:07<09:24,  4.74s/it]
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 202/320 [15:09<07:16,  3.70s/it]
 63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 203/320 [15:10<05:46,  2.96s/it]
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 204/320 [15:11<04:44,  2.45s/it]
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 205/320 [15:12<04:00,  2.09s/it]
 64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 206/320 [15:14<03:29,  1.84s/it]
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 207/320 [15:15<03:08,  1.67s/it]
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 208/320 [15:16<02:52,  1.54s/it]
 65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 209/320 [15:17<02:41,  1.46s/it]
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 210/320 [15:19<02:33,  1.40s/it]
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 211/320 [15:20<02:27,  1.36s/it]
 66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 212/320 [15:21<02:23,  1.33s/it]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 213/320 [15:22<02:19,  1.31s/it]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 214/320 [15:24<02:17,  1.29s/it]
 67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 215/320 [15:25<02:14,  1.28s/it]
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 216/320 [15:26<02:12,  1.28s/it]
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 217/320 [15:27<02:11,  1.27s/it]
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 218/320 [15:29<02:09,  1.27s/it]
 68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 219/320 [15:30<02:08,  1.27s/it]
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 220/320 [15:31<02:07,  1.27s/it]
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 221/320 [15:32<02:05,  1.27s/it]
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 222/320 [15:34<02:04,  1.27s/it]
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 223/320 [15:35<02:03,  1.27s/it]
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 224/320 [15:36<02:01,  1.27s/it]
 70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 225/320 [15:38<02:00,  1.27s/it]
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 226/320 [15:39<01:59,  1.27s/it]
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 227/320 [15:40<01:58,  1.27s/it]
 71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 228/320 [15:41<01:57,  1.27s/it]
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 229/320 [15:43<01:55,  1.27s/it]
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 230/320 [15:44<01:54,  1.28s/it]
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 231/320 [15:45<01:53,  1.28s/it]
 72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 232/320 [15:46<01:52,  1.27s/it]
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 233/320 [15:48<01:50,  1.27s/it]
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 234/320 [15:49<01:49,  1.27s/it]
 73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž  | 235/320 [15:50<01:48,  1.27s/it]
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 236/320 [15:52<01:47,  1.27s/it]
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 237/320 [15:53<01:45,  1.28s/it]
 74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 238/320 [15:54<01:44,  1.28s/it]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 239/320 [15:55<01:43,  1.28s/it]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 240/320 [15:57<01:42,  1.28s/it]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 241/320 [15:58<01:40,  1.28s/it]
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 242/320 [15:59<01:39,  1.28s/it]
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 243/320 [16:01<01:38,  1.28s/it]
 76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 244/320 [16:02<01:37,  1.28s/it]
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 245/320 [16:03<01:35,  1.28s/it]
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 246/320 [16:04<01:34,  1.28s/it]
 77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 247/320 [16:06<01:33,  1.28s/it]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 248/320 [16:07<01:32,  1.28s/it]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 249/320 [16:08<01:30,  1.28s/it]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 250/320 [16:09<01:29,  1.28s/it]
 78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 251/320 [16:11<01:28,  1.28s/it]
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 252/320 [16:12<01:26,  1.28s/it]
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 253/320 [16:13<01:25,  1.28s/it]
 79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 254/320 [16:15<01:24,  1.28s/it]
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 255/320 [16:16<01:23,  1.28s/it]
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 256/320 [16:17<01:12,  1.13s/it]***** Running Evaluation *****
  Num examples = 1004
  Batch size = 64
{'eval_loss': 1.1770559549331665, 'eval_runtime': 5.1002, 'eval_samples_per_second': 196.857, 'eval_steps_per_second': 3.137, 'epoch': 3.0}


  0%|          | 0/16 [00:00<?, ?it/s][A

 12%|â–ˆâ–Ž        | 2/16 [00:00<00:02,  6.21it/s][A

 19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  4.33it/s][A

 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:00<00:03,  3.76it/s][A

 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:03,  3.49it/s][A

 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  3.34it/s][A

 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  3.25it/s][A

 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.20it/s][A

 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:02,  3.16it/s][A

 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.13it/s][A

 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:03<00:01,  3.11it/s][A

 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.10it/s][A

 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.09it/s][A

 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:04<00:00,  3.09it/s][A

 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:04<00:00,  3.09it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.37it/s][A
                                                 

                                               
[A
 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 256/320 [16:22<01:12,  1.13s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.37it/s][A

                                               [ASaving model checkpoint to results/checkpoints/20q/gpt2-xl/combined/checkpoint-256
Configuration saved in results/checkpoints/20q/gpt2-xl/combined/checkpoint-256/config.json
Model weights saved in results/checkpoints/20q/gpt2-xl/combined/checkpoint-256/pytorch_model.bin
Deleting older checkpoint [results/checkpoints/20q/gpt2-xl/combined/checkpoint-192] due to args.save_total_limit

 80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 257/320 [20:27<1:19:48, 76.01s/it]
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 258/320 [20:29<55:21, 53.58s/it]  
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 259/320 [20:30<38:30, 37.88s/it]
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 260/320 [20:31<26:53, 26.89s/it]
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 261/320 [20:32<18:52, 19.20s/it]
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 262/320 [20:34<13:21, 13.81s/it]
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 263/320 [20:35<09:32, 10.05s/it]
 82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 264/320 [20:36<06:54,  7.41s/it]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 265/320 [20:37<05:05,  5.56s/it]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 266/320 [20:39<03:50,  4.27s/it]
 83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž | 267/320 [20:40<02:58,  3.37s/it]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 268/320 [20:41<02:22,  2.73s/it]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 269/320 [20:42<01:56,  2.29s/it]
 84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 270/320 [20:44<01:38,  1.98s/it]
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 271/320 [20:45<01:26,  1.76s/it]
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 272/320 [20:46<01:17,  1.61s/it]
 85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 273/320 [20:47<01:10,  1.51s/it]
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 274/320 [20:49<01:05,  1.43s/it]
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 275/320 [20:50<01:02,  1.38s/it]
 86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 276/320 [20:51<00:59,  1.34s/it]
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 277/320 [20:52<00:56,  1.32s/it]
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 278/320 [20:54<00:54,  1.30s/it]
 87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 279/320 [20:55<00:52,  1.29s/it]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 280/320 [20:56<00:51,  1.28s/it]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 281/320 [20:58<00:49,  1.28s/it]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 282/320 [20:59<00:48,  1.27s/it]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 283/320 [21:00<00:47,  1.27s/it]
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 284/320 [21:01<00:45,  1.27s/it]
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 285/320 [21:03<00:44,  1.27s/it]
 89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 286/320 [21:04<00:43,  1.27s/it]
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 287/320 [21:05<00:41,  1.27s/it]
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 288/320 [21:06<00:40,  1.27s/it]
 90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 289/320 [21:08<00:39,  1.27s/it]
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 290/320 [21:09<00:38,  1.27s/it]
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 291/320 [21:10<00:36,  1.27s/it]
 91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 292/320 [21:11<00:35,  1.27s/it]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 293/320 [21:13<00:34,  1.27s/it]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 294/320 [21:14<00:33,  1.27s/it]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 295/320 [21:15<00:31,  1.27s/it]
 92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 296/320 [21:17<00:30,  1.27s/it]
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 297/320 [21:18<00:29,  1.27s/it]
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 298/320 [21:19<00:28,  1.27s/it]
 93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž| 299/320 [21:20<00:26,  1.27s/it]
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 300/320 [21:22<00:25,  1.27s/it]
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 301/320 [21:23<00:24,  1.27s/it]
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 302/320 [21:24<00:22,  1.27s/it]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 303/320 [21:25<00:21,  1.28s/it]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 304/320 [21:27<00:20,  1.28s/it]
 95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 305/320 [21:28<00:19,  1.28s/it]
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 306/320 [21:29<00:17,  1.28s/it]
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 307/320 [21:31<00:16,  1.28s/it]
 96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 308/320 [21:32<00:15,  1.28s/it]
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 309/320 [21:33<00:14,  1.28s/it]
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 310/320 [21:34<00:12,  1.28s/it]
 97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 311/320 [21:36<00:11,  1.28s/it]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 312/320 [21:37<00:10,  1.28s/it]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 313/320 [21:38<00:08,  1.28s/it]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 314/320 [21:40<00:07,  1.28s/it]
 98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 315/320 [21:41<00:06,  1.28s/it]
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 316/320 [21:42<00:05,  1.28s/it]
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 317/320 [21:43<00:03,  1.28s/it]
 99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 318/320 [21:45<00:02,  1.28s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 319/320 [21:46<00:01,  1.28s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 320/320 [21:47<00:00,  1.13s/it]***** Running Evaluation *****
  Num examples = 1004
  Batch size = 64
{'eval_loss': 1.2427932024002075, 'eval_runtime': 5.1029, 'eval_samples_per_second': 196.75, 'eval_steps_per_second': 3.135, 'epoch': 4.0}


  0%|          | 0/16 [00:00<?, ?it/s][A

 12%|â–ˆâ–Ž        | 2/16 [00:00<00:02,  6.20it/s][A

 19%|â–ˆâ–‰        | 3/16 [00:00<00:03,  4.33it/s][A

 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:00<00:03,  3.77it/s][A

 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:03,  3.51it/s][A

 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  3.35it/s][A

 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  3.26it/s][A

 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.21it/s][A

 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:02,  3.17it/s][A

 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.14it/s][A

 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:03<00:01,  3.11it/s][A

 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.10it/s][A

 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.09it/s][A

 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:04<00:00,  3.09it/s][A

 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:04<00:00,  3.09it/s][A

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.37it/s][A
                                                 

                                               
[A
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 320/320 [21:52<00:00,  1.13s/it]

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.37it/s][A

                                               [ASaving model checkpoint to results/checkpoints/20q/gpt2-xl/combined/checkpoint-320
Configuration saved in results/checkpoints/20q/gpt2-xl/combined/checkpoint-320/config.json
Model weights saved in results/checkpoints/20q/gpt2-xl/combined/checkpoint-320/pytorch_model.bin
Deleting older checkpoint [results/checkpoints/20q/gpt2-xl/combined/checkpoint-256] due to args.save_total_limit


Training completed. Do not forget to share your model on huggingface.co/models =)


Loading best model from results/checkpoints/20q/gpt2-xl/combined/checkpoint-64 (score: 1.0134471654891968).

                                                 

100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 320/320 [27:19<00:00,  1.13s/it]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 320/320 [27:19<00:00,  5.12s/it]
***** Running Evaluation *****
  Num examples = 1004
  Batch size = 64
{'eval_loss': 1.316015362739563, 'eval_runtime': 5.0966, 'eval_samples_per_second': 196.993, 'eval_steps_per_second': 3.139, 'epoch': 5.0}
{'train_runtime': 1639.5018, 'train_samples_per_second': 12.382, 'train_steps_per_second': 0.195, 'train_loss': 0.6222230911254882, 'epoch': 5.0}

  0%|          | 0/16 [00:00<?, ?it/s]
 12%|â–ˆâ–Ž        | 2/16 [00:00<00:02,  6.44it/s]
 19%|â–ˆâ–‰        | 3/16 [00:00<00:02,  4.52it/s]
 25%|â–ˆâ–ˆâ–Œ       | 4/16 [00:00<00:03,  3.91it/s]
 31%|â–ˆâ–ˆâ–ˆâ–      | 5/16 [00:01<00:03,  3.63it/s]
 38%|â–ˆâ–ˆâ–ˆâ–Š      | 6/16 [00:01<00:02,  3.48it/s]
 44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 7/16 [00:01<00:02,  3.38it/s]
 50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 8/16 [00:02<00:02,  3.32it/s]
 56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 9/16 [00:02<00:02,  3.28it/s]
 62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Ž   | 10/16 [00:02<00:01,  3.24it/s]
 69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 11/16 [00:03<00:01,  3.22it/s]
 75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 12/16 [00:03<00:01,  3.21it/s]
 81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 13/16 [00:03<00:00,  3.20it/s]
 88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 14/16 [00:04<00:00,  3.20it/s]
 94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 15/16 [00:04<00:00,  3.20it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.49it/s]
100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 16/16 [00:04<00:00,  3.46it/s]
Saving model checkpoint to results/best-checkpoints/20q/gpt2-xl/combined
Configuration saved in results/best-checkpoints/20q/gpt2-xl/combined/config.json
Model weights saved in results/best-checkpoints/20q/gpt2-xl/combined/pytorch_model.bin
loading configuration file results/best-checkpoints/20q/gpt2-xl/combined/config.json
Model config GPT2Config {
  "_name_or_path": "results/best-checkpoints/20q/gpt2-xl/combined",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1600,
  "n_head": 25,
  "n_inner": null,
  "n_layer": 48,
  "n_positions": 1024,
  "output_past": true,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "torch_dtype": "float32",
  "transformers_version": "4.23.1",
  "use_cache": true,
  "vocab_size": 50257
}

loading weights file results/best-checkpoints/20q/gpt2-xl/combined/pytorch_model.bin
All model checkpoint weights were used when initializing GPT2LMHeadModel.

All the weights of GPT2LMHeadModel were initialized from the model checkpoint at results/best-checkpoints/20q/gpt2-xl/combined.
If your task is similar to the task the model of the checkpoint was trained on, you can already use GPT2LMHeadModel for predictions without further training.
Could not locate the tokenizer configuration file, will try to use the model config instead.
loading configuration file config.json from cache at /work/anshitagupta_umass_edu/.cache/huggingface/hub/models--gpt2-xl/snapshots/33cdb5c0db5423c1879b1b9f16c352988e8754a8/config.json
Model config GPT2Config {
  "_name_or_path": "gpt2-xl",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1600,
  "n_head": 25,
  "n_inner": null,
  "n_layer": 48,
  "n_positions": 1024,
  "output_past": true,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.23.1",
  "use_cache": true,
  "vocab_size": 50257
}

loading file vocab.json from cache at /work/anshitagupta_umass_edu/.cache/huggingface/hub/models--gpt2-xl/snapshots/33cdb5c0db5423c1879b1b9f16c352988e8754a8/vocab.json
loading file merges.txt from cache at /work/anshitagupta_umass_edu/.cache/huggingface/hub/models--gpt2-xl/snapshots/33cdb5c0db5423c1879b1b9f16c352988e8754a8/merges.txt
loading file tokenizer.json from cache at /work/anshitagupta_umass_edu/.cache/huggingface/hub/models--gpt2-xl/snapshots/33cdb5c0db5423c1879b1b9f16c352988e8754a8/tokenizer.json
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at None
loading file tokenizer_config.json from cache at None
loading configuration file config.json from cache at /work/anshitagupta_umass_edu/.cache/huggingface/hub/models--gpt2-xl/snapshots/33cdb5c0db5423c1879b1b9f16c352988e8754a8/config.json
Model config GPT2Config {
  "_name_or_path": "gpt2-xl",
  "activation_function": "gelu_new",
  "architectures": [
    "GPT2LMHeadModel"
  ],
  "attn_pdrop": 0.1,
  "bos_token_id": 50256,
  "embd_pdrop": 0.1,
  "eos_token_id": 50256,
  "initializer_range": 0.02,
  "layer_norm_epsilon": 1e-05,
  "model_type": "gpt2",
  "n_ctx": 1024,
  "n_embd": 1600,
  "n_head": 25,
  "n_inner": null,
  "n_layer": 48,
  "n_positions": 1024,
  "output_past": true,
  "reorder_and_upcast_attn": false,
  "resid_pdrop": 0.1,
  "scale_attn_by_inverse_layer_idx": false,
  "scale_attn_weights": true,
  "summary_activation": null,
  "summary_first_dropout": 0.1,
  "summary_proj_to_labels": true,
  "summary_type": "cls_index",
  "summary_use_proj": true,
  "task_specific_params": {
    "text-generation": {
      "do_sample": true,
      "max_length": 50
    }
  },
  "transformers_version": "4.23.1",
  "use_cache": true,
  "vocab_size": 50257
}

{'eval_loss': 1.0134471654891968, 'eval_runtime': 4.9447, 'eval_samples_per_second': 203.048, 'eval_steps_per_second': 3.236, 'epoch': 5.0}
Loaded dataset with 4060 elements
Loaded dataset with 1004 elements
Loaded dataset with 5055 elements
train Accuracy =  0.8369458128078818
train F1 score =  0.6011407732134714
train Confusion Matrix = 
 [[1475   46  493]
 [   0 1923  123]
 [   0    0    0]]
Classification Report: 
               precision    recall  f1-score   support

        True       1.00      0.73      0.85      2014
       False       0.98      0.94      0.96      2046
        None       0.00      1.00      0.00         0

    accuracy                           0.84      4060
   macro avg       0.66      0.89      0.60      4060
weighted avg       0.99      0.84      0.90      4060

valid Accuracy =  0.8645418326693227
valid F1 score =  0.6152085774859956
valid Confusion Matrix = 
 [[393   5 104]
 [  0 475  27]
 [  0   0   0]]
Classification Report: 
               precision    recall  f1-score   support

        True       1.00      0.78      0.88       502
       False       0.99      0.95      0.97       502
        None       0.00      1.00      0.00         0

    accuracy                           0.86      1004
   macro avg       0.66      0.91      0.62      1004
weighted avg       0.99      0.86      0.92      1004

test Accuracy =  0.8340257171117705
test F1 score =  0.5998644434832512
test Confusion Matrix = 
 [[1847   51  609]
 [  13 2369  166]
 [   0    0    0]]
Classification Report: 
               precision    recall  f1-score   support

        True       0.99      0.74      0.85      2507
       False       0.98      0.93      0.95      2548
        None       0.00      1.00      0.00         0

    accuracy                           0.83      5055
   macro avg       0.66      0.89      0.60      5055
weighted avg       0.99      0.83      0.90      5055

